{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "add45ee2-0f32-417f-b4f6-753ee326fb93",
   "metadata": {},
   "source": [
    "# pplm_discrim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af42f93b-2f7a-469e-99c4-93809ecd8093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from pplm_classification_head import ClassificationHead\n",
    "from torch import nn\n",
    "from torchtext import data as torchtext_data\n",
    "from torchtext import datasets\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "EPSILON = 1e-10\n",
    "example_sentence = \"This is incredible! I love it, this is the best chicken I have ever had.\"\n",
    "max_length_seq = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83db4b50-0c71-4776-bbe4-a9b726cffe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Transformer encoder followed by a Classification Head\"\"\"\n",
    "\n",
    "    def __init__(self, class_size, pretrained_model=\"llama\", cached_mode=False, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer = AutoTokenizer.from_pretrained(\"/home/wooseok/llama-7b-hf\")\n",
    "        self.encoder = AutoModelForCausalLM.from_pretrained(\"/home/wooseok/llama-7b-hf\", device_map=\"auto\", load_in_8bit=True)\n",
    "        self.embed_size = self.encoder.model.config.hidden_size\n",
    "        self.classifier_head = ClassificationHead(class_size=class_size, embed_size=self.embed_size)\n",
    "        self.cached_mode = cached_mode\n",
    "        self.device = device\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.classifier_head\n",
    "\n",
    "    def train_custom(self):\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.classifier_head.train()\n",
    "\n",
    "    def avg_representation(self, x):\n",
    "        mask = x.ne(0).unsqueeze(2).repeat(1, 1, self.embed_size).float().to(self.device).detach()\n",
    "        hidden = self.encoder.model(x)[\"last_hidden_state\"]\n",
    "        masked_hidden = hidden * mask\n",
    "        avg_hidden = torch.sum(masked_hidden, dim=1) / (torch.sum(mask, dim=1).detach() + EPSILON)\n",
    "        return avg_hidden\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.cached_mode:\n",
    "            avg_hidden = x.to(self.device)\n",
    "        else:\n",
    "            avg_hidden = self.avg_representation(x.to(self.device))\n",
    "\n",
    "        logits = self.classifier_head(avg_hidden)\n",
    "        probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "\n",
    "        return probs\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"Reads source and target sequences from txt files.\"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Returns one data pair (source and target).\"\"\"\n",
    "        data = {}\n",
    "        data[\"X\"] = self.X[index]\n",
    "        data[\"y\"] = self.y[index]\n",
    "        return data\n",
    "def cached_collate_fn(data):\n",
    "    item_info = {}\n",
    "    for key in data[0].keys():\n",
    "        item_info[key] = [d[key] for d in data]\n",
    "\n",
    "    x_batch = torch.cat(item_info[\"X\"], 0)\n",
    "    y_batch = torch.tensor(item_info[\"y\"], dtype=torch.long)\n",
    "\n",
    "    return x_batch, y_batch\n",
    "def collate_fn(data):\n",
    "    def pad_sequences(sequences):\n",
    "        lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "        padded_sequences = torch.zeros(len(sequences), max(lengths)).long()  # padding value = 0\n",
    "\n",
    "        for i, seq in enumerate(sequences):\n",
    "            end = lengths[i]\n",
    "            padded_sequences[i, :end] = seq[:end]\n",
    "\n",
    "        return padded_sequences, lengths\n",
    "\n",
    "    item_info = {}\n",
    "    for key in data[0].keys():\n",
    "        item_info[key] = [d[key] for d in data]\n",
    "\n",
    "    x_batch, _ = pad_sequences(item_info[\"X\"])\n",
    "    y_batch = torch.tensor(item_info[\"y\"], dtype=torch.long)\n",
    "\n",
    "    return x_batch, y_batch\n",
    "def train_epoch(data_loader, discriminator, optimizer, epoch=0, log_interval=10, device=\"cpu\"):\n",
    "    samples_so_far = 0\n",
    "    discriminator.train_custom()\n",
    "    for batch_idx, (input_t, target_t) in enumerate(data_loader):\n",
    "        input_t, target_t = input_t.to(device), target_t.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_t = discriminator(input_t)\n",
    "        loss = nn.functional.nll_loss(output_t, target_t)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        samples_so_far += len(input_t)\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch + 1,\n",
    "                    samples_so_far,\n",
    "                    len(data_loader.dataset),\n",
    "                    100 * samples_so_far / len(data_loader.dataset),\n",
    "                    loss.item(),\n",
    "                )\n",
    "            )\n",
    "def get_cached_data_loader(dataset, batch_size, discriminator, shuffle=False, device=\"cpu\"):\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for batch_idx, (x, y) in enumerate(tqdm(data_loader, ascii=True)):\n",
    "        with torch.no_grad():\n",
    "            x = x.to(device)\n",
    "            avg_rep = discriminator.avg_representation(x).cpu().detach()\n",
    "            avg_rep_list = torch.unbind(avg_rep.unsqueeze(1))\n",
    "            xs += avg_rep_list\n",
    "            ys += y.cpu().numpy().tolist()\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=Dataset(xs, ys), batch_size=batch_size, shuffle=shuffle, collate_fn=cached_collate_fn\n",
    "    )\n",
    "\n",
    "    return data_loader, xs, ys\n",
    "def evaluate_performance(data_loader, discriminator, device=\"cpu\"):\n",
    "    discriminator.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for input_t, target_t in data_loader:\n",
    "            input_t, target_t = input_t.to(device), target_t.to(device)\n",
    "            output_t = discriminator(input_t)\n",
    "            # sum up batch loss\n",
    "            test_loss += nn.functional.nll_loss(output_t, target_t, reduction=\"sum\").item()\n",
    "            # get the index of the max log-probability\n",
    "            pred_t = output_t.argmax(dim=1, keepdim=True)\n",
    "            correct += pred_t.eq(target_t.view_as(pred_t)).sum().item()\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"Performance on test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format(\n",
    "            test_loss, correct, len(data_loader.dataset), 100.0 * correct / len(data_loader.dataset)\n",
    "        )\n",
    "    )\n",
    "\n",
    "def predict(input_sentence, model, classes, cached=False, device=\"cpu\"):\n",
    "    input_t = model.tokenizer.encode(input_sentence)\n",
    "    input_t = torch.tensor([input_t], dtype=torch.long, device=device)\n",
    "    if cached:\n",
    "        input_t = model.avg_representation(input_t)\n",
    "\n",
    "    log_probs = model(input_t).data.cpu().numpy().flatten().tolist()\n",
    "    print(\"Input sentence:\", input_sentence)\n",
    "    print(\n",
    "        \"Predictions:\",\n",
    "        \", \".join(\"{}: {:.4f}\".format(c, math.exp(log_prob)) for c, log_prob in zip(classes, log_probs)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307cc024-b32d-422f-b992-d3d23a6dce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/wooseok/llama-7b-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/home/wooseok/llama-7b-hf\", device_map=\"auto\", load_in_8bit=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33e24c77-813f-4018-8ebe-7ff336c4f6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n",
      "/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /home/wooseok/miniconda3/envs/mh/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda113_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████| 33/33 [00:11<00:00,  2.92it/s]\n"
     ]
    }
   ],
   "source": [
    "dicsriminator = Discriminator(class_size=5, pretrained_model=\"llama\", cached_mode=False, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc3c84d0-de7d-4ce9-bd00-92c26fb87250",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_inputs = dicsriminator.tokenizer('My dog died today', return_tensors='pt')\n",
    "encoded_inputs = { k: v.to('cuda') for k, v in encoded_inputs.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34fc827f-193d-44a4-8e55-1379ba1d868e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,  1619, 11203,  6423,  9826]], device='cuda:0'),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d290bd6-d958-4a40-8fb6-87b226c56873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_inputs['input_ids'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9183e389-e048-42e5-92da-2808b87d99e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.LongTensor([[50264, 1, 1619, 11203, 6423, 9826]]).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e186e84d-1e6a-4835-b70d-5cb9a3eca6e0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [96,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [97,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [98,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [99,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [100,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [101,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [102,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [103,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [104,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [105,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [106,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [107,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [108,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [109,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [110,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [111,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [112,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [113,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [114,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [115,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [116,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [117,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [118,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [119,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [120,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [121,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [122,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [123,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [124,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [125,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [126,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [127,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [4,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [8,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [19,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 outputs = dicsriminator.encoder(x, output_hidden_states=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1102</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1099 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1100 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1101 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1102 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1103 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1104 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1105 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">hooks.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">165</span> in          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">new_forward</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">162 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 │   │   │   │   </span>output = old_forward(*args, **kwargs)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">164 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>165 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output = old_forward(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">166 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> module._hf_hook.post_forward(module, output)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">167 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 │   </span>module.forward = new_forward                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/transformers/models/llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">llama.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">772</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">769 │   │   </span>return_dict = return_dict <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> return_dict <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.use_return   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">770 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">771 │   │   # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>772 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model(                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">773 │   │   │   </span>input_ids=input_ids,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">774 │   │   │   </span>attention_mask=attention_mask,                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">775 │   │   │   </span>past_key_values=past_key_values,                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1102</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1099 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1100 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1101 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1102 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1103 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1104 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1105 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">hooks.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">165</span> in          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">new_forward</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">162 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 │   │   │   │   </span>output = old_forward(*args, **kwargs)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">164 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>165 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output = old_forward(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">166 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> module._hf_hook.post_forward(module, output)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">167 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 │   </span>module.forward = new_forward                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/transformers/models/llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">llama.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">581</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">578 │   │   │   </span>attention_mask = torch.ones(                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">579 │   │   │   │   </span>(batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embe   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">580 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>581 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>attention_mask = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._prepare_decoder_attention_mask(                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">582 │   │   │   </span>attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_len   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">583 │   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">584 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/transformers/models/llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">llama.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">483</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_prepare_decoder_attention_mask</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">480 │   │   # [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]</span>                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">481 │   │   </span>combined_attention_mask = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">482 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> input_shape[-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>] &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>:                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>483 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>combined_attention_mask = _make_causal_mask(                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">484 │   │   │   │   </span>input_shape, inputs_embeds.dtype, past_key_values_length=past_key_values   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">485 │   │   │   </span>).to(inputs_embeds.device)                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">486 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>CUDA error: device-side assert triggered\n",
       "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be \n",
       "incorrect.\n",
       "For debugging consider passing <span style=\"color: #808000; text-decoration-color: #808000\">CUDA_LAUNCH_BLOCKING</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 outputs = dicsriminator.encoder(x, output_hidden_states=\u001b[94mTrue\u001b[0m)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1102\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1099 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1100 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1101 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1102 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1103 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1104 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1105 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/accelerate/\u001b[0m\u001b[1;33mhooks.py\u001b[0m:\u001b[94m165\u001b[0m in          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mnew_forward\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m162 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m165 \u001b[2m│   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m166 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m module._hf_hook.post_forward(module, output)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m167 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   \u001b[0mmodule.forward = new_forward                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/transformers/models/llama/\u001b[0m\u001b[1;33mmodeling_\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mllama.py\u001b[0m:\u001b[94m772\u001b[0m in \u001b[92mforward\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m769 \u001b[0m\u001b[2m│   │   \u001b[0mreturn_dict = return_dict \u001b[94mif\u001b[0m return_dict \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.config.use_return   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m770 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m771 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m772 \u001b[2m│   │   \u001b[0moutputs = \u001b[96mself\u001b[0m.model(                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m773 \u001b[0m\u001b[2m│   │   │   \u001b[0minput_ids=input_ids,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m774 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=attention_mask,                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m775 \u001b[0m\u001b[2m│   │   │   \u001b[0mpast_key_values=past_key_values,                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1102\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1099 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1100 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1101 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1102 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1103 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1104 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1105 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/accelerate/\u001b[0m\u001b[1;33mhooks.py\u001b[0m:\u001b[94m165\u001b[0m in          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mnew_forward\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m162 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m165 \u001b[2m│   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m166 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m module._hf_hook.post_forward(module, output)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m167 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   \u001b[0mmodule.forward = new_forward                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/transformers/models/llama/\u001b[0m\u001b[1;33mmodeling_\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mllama.py\u001b[0m:\u001b[94m581\u001b[0m in \u001b[92mforward\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m578 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask = torch.ones(                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m579 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m(batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embe   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m580 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m581 \u001b[2m│   │   \u001b[0mattention_mask = \u001b[96mself\u001b[0m._prepare_decoder_attention_mask(                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m582 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_len   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m583 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m584 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/transformers/models/llama/\u001b[0m\u001b[1;33mmodeling_\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mllama.py\u001b[0m:\u001b[94m483\u001b[0m in \u001b[92m_prepare_decoder_attention_mask\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m480 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\u001b[0m                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m481 \u001b[0m\u001b[2m│   │   \u001b[0mcombined_attention_mask = \u001b[94mNone\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m482 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m input_shape[-\u001b[94m1\u001b[0m] > \u001b[94m1\u001b[0m:                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m483 \u001b[2m│   │   │   \u001b[0mcombined_attention_mask = _make_causal_mask(                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m484 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minput_shape, inputs_embeds.dtype, past_key_values_length=past_key_values   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m485 \u001b[0m\u001b[2m│   │   │   \u001b[0m).to(inputs_embeds.device)                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m486 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0mCUDA error: device-side assert triggered\n",
       "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be \n",
       "incorrect.\n",
       "For debugging consider passing \u001b[33mCUDA_LAUNCH_BLOCKING\u001b[0m=\u001b[1;36m1\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = dicsriminator.encoder(x, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8d66c78-3f0f-478a-8eb2-e8ea4f6671d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dicsriminator.tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a99d3926-07ef-46c3-8662-8b7a92920a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = dicsriminator.encoder(**encoded_inputs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0491652b-236f-499f-8e38-1f12670e8bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'past_key_values', 'hidden_states'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51d69aea-8015-4b64-bc95-e9d590d883c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 4096])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b1c4ed-bcd7-4ca2-8cf1-814964903679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92bae8d-d5c0-4af6-a8ce-c802adfca301",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_output = model(past_key_values=curr_unpert_past, inputs_embeds=x,return_dict=True,output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1e5a23-5144-4ff7-b6b9-c31bc4d094ac",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a621faa-32b2-4c6e-a55e-458b73d2235e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing SST dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /home/wooseok/miniconda3/envs/mh/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 6.1\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda113_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n",
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.backends' has no attribute 'mps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m idx2class \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositive\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvery positive\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvery negative\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mneutral\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     12\u001b[0m class2idx \u001b[38;5;241m=\u001b[39m {c: i \u001b[38;5;28;01mfor\u001b[39;00m i, c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(idx2class)}\n\u001b[0;32m---> 14\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m \u001b[43mDiscriminator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx2class\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcached_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     19\u001b[0m text \u001b[38;5;241m=\u001b[39m torchtext_data\u001b[38;5;241m.\u001b[39mField()\n\u001b[1;32m     20\u001b[0m label \u001b[38;5;241m=\u001b[39m torchtext_data\u001b[38;5;241m.\u001b[39mField(sequential\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m, in \u001b[0;36mDiscriminator.__init__\u001b[0;34m(self, class_size, pretrained_model, cached_mode, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/wooseok/llama-7b-hf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/wooseok/llama-7b-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier_head \u001b[38;5;241m=\u001b[39m ClassificationHead(class_size\u001b[38;5;241m=\u001b[39mclass_size, embed_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_size)\n",
      "File \u001b[0;32m~/miniconda3/envs/mh/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:471\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    470\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    477\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mh/lib/python3.8/site-packages/transformers/modeling_utils.py:2556\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`device_map=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_map\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` requires a source install of Accelerate.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msequential\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m get_balanced_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2556\u001b[0m     max_memory \u001b[38;5;241m=\u001b[39m \u001b[43mget_balanced_memory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_split_module_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_split_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_zero\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbalanced_low_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2562\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;66;03m# Make sure tied weights are tied before creating the device map.\u001b[39;00m\n\u001b[1;32m   2564\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m~/miniconda3/envs/mh/lib/python3.8/site-packages/accelerate/utils/modeling.py:731\u001b[0m, in \u001b[0;36mget_balanced_memory\u001b[0;34m(model, max_memory, no_split_module_classes, dtype, special_dtypes, low_zero)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;124;03mCompute a `max_memory` dictionary for [`infer_auto_device_map`] that will balance the use of each available GPU.\u001b[39;00m\n\u001b[1;32m    705\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[38;5;124;03m        Transformers generate function).\u001b[39;00m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    730\u001b[0m \u001b[38;5;66;03m# Get default / clean up max_memory\u001b[39;00m\n\u001b[0;32m--> 731\u001b[0m max_memory \u001b[38;5;241m=\u001b[39m \u001b[43mget_max_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mor\u001b[39;00m is_xpu_available()) \u001b[38;5;129;01mor\u001b[39;00m is_mps_available():\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m max_memory\n",
      "File \u001b[0;32m~/miniconda3/envs/mh/lib/python3.8/site-packages/accelerate/utils/modeling.py:631\u001b[0m, in \u001b[0;36mget_max_memory\u001b[0;34m(max_memory)\u001b[0m\n\u001b[1;32m    629\u001b[0m         max_memory \u001b[38;5;241m=\u001b[39m {i: torch\u001b[38;5;241m.\u001b[39mxpu\u001b[38;5;241m.\u001b[39mmax_memory_allocated(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(torch\u001b[38;5;241m.\u001b[39mxpu\u001b[38;5;241m.\u001b[39mdevice_count())}\n\u001b[1;32m    630\u001b[0m \u001b[38;5;66;03m# allocate everything in the mps device as the RAM is shared\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_mps_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    632\u001b[0m     max_memory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m psutil\u001b[38;5;241m.\u001b[39mvirtual_memory()\u001b[38;5;241m.\u001b[39mavailable\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mh/lib/python3.8/site-packages/accelerate/utils/imports.py:199\u001b[0m, in \u001b[0;36mis_mps_available\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_mps_available\u001b[39m():\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m is_torch_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.12\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmps\u001b[49m\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39mis_built()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.backends' has no attribute 'mps'"
     ]
    }
   ],
   "source": [
    "dataset = \"SST\"\n",
    "no_cuda =False\n",
    "device = \"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\"\n",
    "pretrained_model = \"llama\"\n",
    "cached= True\n",
    "\n",
    "\n",
    "print(\"Preprocessing {} dataset...\".format(dataset))\n",
    "start = time.time()\n",
    "\n",
    "idx2class = [\"positive\", \"negative\", \"very positive\", \"very negative\", \"neutral\"]\n",
    "class2idx = {c: i for i, c in enumerate(idx2class)}\n",
    "\n",
    "discriminator = Discriminator(\n",
    "    class_size=len(idx2class), pretrained_model=pretrained_model, cached_mode=cached, device=device\n",
    ").to(device)\n",
    "\n",
    "\n",
    "text = torchtext_data.Field()\n",
    "label = torchtext_data.Field(sequential=False)\n",
    "train_data, val_data, test_data = datasets.SST.splits(\n",
    "    text,\n",
    "    label,\n",
    "    fine_grained=True,\n",
    "    train_subtrees=True,\n",
    ")\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "for i in trange(len(train_data), ascii=True):\n",
    "    seq = TreebankWordDetokenizer().detokenize(vars(train_data[i])[\"text\"])\n",
    "    seq = discriminator.tokenizer.encode(seq)\n",
    "    #seq = torch.tensor([50256] + seq, device='cpu', dtype=torch.long)\n",
    "    seq = torch.tensor(seq, device='cpu', dtype=torch.long)\n",
    "    if len(seq)< 60 and len(seq)> 15 :\n",
    "        x.append(seq)\n",
    "        y.append(class2idx[vars(train_data[i])[\"label\"]])\n",
    "train_dataset = Dataset(x, y)\n",
    "\n",
    "test_x = []\n",
    "test_y = []\n",
    "for i in trange(len(test_data), ascii=True):\n",
    "    seq = TreebankWordDetokenizer().detokenize(vars(test_data[i])[\"text\"])\n",
    "    seq = discriminator.tokenizer.encode(seq)\n",
    "    #seq = torch.tensor([50256] + seq, device='cpu', dtype=torch.long)\n",
    "    seq = torch.tensor(seq, device='cpu', dtype=torch.long)\n",
    "    test_x.append(seq)\n",
    "    test_y.append(class2idx[vars(test_data[i])[\"label\"]])\n",
    "test_dataset = Dataset(test_x, test_y)\n",
    "\n",
    "discriminator_meta = {\n",
    "    \"class_size\": len(idx2class),\n",
    "    \"embed_size\": discriminator.embed_size,\n",
    "    \"pretrained_model\": pretrained_model,\n",
    "    \"class_vocab\": class2idx,\n",
    "    \"default_class\": 2,\n",
    "}\n",
    "end = time.time()\n",
    "print(\"Preprocessed {} data points\".format(len(train_dataset) + len(test_dataset)))\n",
    "print(\"Data preprocessing took: {:.3f}s\".format(end - start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d21419b3-2afb-402c-a235-e213d4134078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34345"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3952ff86-0e01-45f6-8ae5-4dce6e4b3c83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building representation cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|###################################| 1074/1074 [57:42<00:00,  3.22s/it]\n",
      "100%|#######################################| 70/70 [04:35<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building representation cache took: 3737.899s\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "save_model = True\n",
    "epochs = 10\n",
    "log_interval =10 \n",
    "\n",
    "if cached:\n",
    "    print(\"Building representation cache...\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    train_loader,x_tr,y_tr = get_cached_data_loader(train_dataset, batch_size, discriminator, shuffle=True, device=device)\n",
    "\n",
    "    test_loader,x_te,y_te = get_cached_data_loader(test_dataset, batch_size, discriminator, device=device)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Building representation cache took: {:.3f}s\".format(end - start))\n",
    "else:\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n",
    "    \n",
    "with open('x_tr.pkl','wb') as f:\n",
    "    pickle.dump(x_tr,f)\n",
    "with open('y_tr.pkl','wb') as f:\n",
    "    pickle.dump(y_tr,f)\n",
    "with open('x_te.pkl','wb') as f:\n",
    "    pickle.dump(x_te,f)\n",
    "with open('y_te.pkl','wb') as f:\n",
    "    pickle.dump(y_te,f)\n",
    "    \n",
    "\n",
    "if save_model:\n",
    "    with open(\"{}_classifier_head_meta.json\".format(dataset), \"w\") as meta_file:\n",
    "        json.dump(discriminator_meta, meta_file)\n",
    "\n",
    "optimizer = optim.Adam(discriminator.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10d6af61-f917-43df-844f-b4000e5466f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5d2a5da-f453-4795-9c73-7ed6d4fe6b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1df0f009-ab23-4c1c-80b4-abba529c835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70e8c177-3b39-4a6d-a123-7627ee0e126c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 discriminator.encoder(x)                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1102</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1099 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1100 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1101 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1102 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1103 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1104 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1105 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">hooks.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">165</span> in          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">new_forward</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">162 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 │   │   │   │   </span>output = old_forward(*args, **kwargs)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">164 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>165 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output = old_forward(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">166 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> module._hf_hook.post_forward(module, output)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">167 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 │   </span>module.forward = new_forward                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/transformers/models/llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">llama.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">772</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">769 │   │   </span>return_dict = return_dict <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> return_dict <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.use_return   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">770 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">771 │   │   # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)</span>    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>772 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.model(                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">773 │   │   │   </span>input_ids=input_ids,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">774 │   │   │   </span>attention_mask=attention_mask,                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">775 │   │   │   </span>past_key_values=past_key_values,                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1102</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1099 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1100 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1101 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1102 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1103 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1104 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1105 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">hooks.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">165</span> in          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">new_forward</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">162 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 │   │   │   │   </span>output = old_forward(*args, **kwargs)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">164 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>165 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output = old_forward(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">166 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> module._hf_hook.post_forward(module, output)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">167 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 │   </span>module.forward = new_forward                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/transformers/models/llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">llama.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">575</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">572 │   │   │   </span>past_key_values_length = past_key_values[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>][<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>].shape[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>]                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">573 │   │   │   </span>seq_length_with_past = seq_length_with_past + past_key_values_length           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">574 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> inputs_embeds <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>575 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>inputs_embeds = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.embed_tokens(input_ids)                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">576 │   │   # embed positions</span>                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">577 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> attention_mask <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">578 │   │   │   </span>attention_mask = torch.ones(                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1102</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1099 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1100 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1101 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1102 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1103 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1104 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1105 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">hooks.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">165</span> in          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">new_forward</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">162 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 │   │   │   │   </span>output = old_forward(*args, **kwargs)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">164 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>165 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output = old_forward(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">166 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> module._hf_hook.post_forward(module, output)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">167 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 │   </span>module.forward = new_forward                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">sparse.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">158</span> in   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">155 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.weight[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.padding_idx].fill_(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">156 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">157 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>: Tensor) -&gt; Tensor:                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>158 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> F.embedding(                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">159 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.weight, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.padding_idx, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.max_norm,                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">160 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.norm_type, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scale_grad_by_freq, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.sparse)                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">161 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/torch/nn/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">functional.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2044</span> in      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">embedding</span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2041 │   │   #   torch.embedding_renorm_</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2042 │   │   # remove once script supports set_grad_enabled</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2043 │   │   </span>_no_grad_embedding_renorm_(weight, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, max_norm, norm_type)                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2044 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> torch.embedding(weight, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, padding_idx, scale_grad_by_freq, sparse)        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2045 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2046 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2047 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">embedding_bag</span>(                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>CUDA error: device-side assert triggered\n",
       "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be \n",
       "incorrect.\n",
       "For debugging consider passing <span style=\"color: #808000; text-decoration-color: #808000\">CUDA_LAUNCH_BLOCKING</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 discriminator.encoder(x)                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1102\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1099 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1100 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1101 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1102 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1103 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1104 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1105 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/accelerate/\u001b[0m\u001b[1;33mhooks.py\u001b[0m:\u001b[94m165\u001b[0m in          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mnew_forward\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m162 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m165 \u001b[2m│   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m166 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m module._hf_hook.post_forward(module, output)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m167 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   \u001b[0mmodule.forward = new_forward                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/transformers/models/llama/\u001b[0m\u001b[1;33mmodeling_\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mllama.py\u001b[0m:\u001b[94m772\u001b[0m in \u001b[92mforward\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m769 \u001b[0m\u001b[2m│   │   \u001b[0mreturn_dict = return_dict \u001b[94mif\u001b[0m return_dict \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.config.use_return   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m770 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m771 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m772 \u001b[2m│   │   \u001b[0moutputs = \u001b[96mself\u001b[0m.model(                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m773 \u001b[0m\u001b[2m│   │   │   \u001b[0minput_ids=input_ids,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m774 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=attention_mask,                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m775 \u001b[0m\u001b[2m│   │   │   \u001b[0mpast_key_values=past_key_values,                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1102\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1099 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1100 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1101 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1102 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1103 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1104 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1105 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/accelerate/\u001b[0m\u001b[1;33mhooks.py\u001b[0m:\u001b[94m165\u001b[0m in          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mnew_forward\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m162 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m165 \u001b[2m│   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m166 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m module._hf_hook.post_forward(module, output)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m167 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   \u001b[0mmodule.forward = new_forward                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/transformers/models/llama/\u001b[0m\u001b[1;33mmodeling_\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mllama.py\u001b[0m:\u001b[94m575\u001b[0m in \u001b[92mforward\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m572 \u001b[0m\u001b[2m│   │   │   \u001b[0mpast_key_values_length = past_key_values[\u001b[94m0\u001b[0m][\u001b[94m0\u001b[0m].shape[\u001b[94m2\u001b[0m]                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m573 \u001b[0m\u001b[2m│   │   │   \u001b[0mseq_length_with_past = seq_length_with_past + past_key_values_length           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m574 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m inputs_embeds \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m575 \u001b[2m│   │   │   \u001b[0minputs_embeds = \u001b[96mself\u001b[0m.embed_tokens(input_ids)                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m576 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# embed positions\u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m577 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m attention_mask \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m578 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask = torch.ones(                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1102\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1099 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1100 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1101 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1102 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1103 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1104 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1105 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/accelerate/\u001b[0m\u001b[1;33mhooks.py\u001b[0m:\u001b[94m165\u001b[0m in          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mnew_forward\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m162 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m165 \u001b[2m│   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m166 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m module._hf_hook.post_forward(module, output)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m167 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   \u001b[0mmodule.forward = new_forward                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33msparse.py\u001b[0m:\u001b[94m158\u001b[0m in   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mforward\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m155 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.weight[\u001b[96mself\u001b[0m.padding_idx].fill_(\u001b[94m0\u001b[0m)                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m156 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m157 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, \u001b[96minput\u001b[0m: Tensor) -> Tensor:                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m158 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m F.embedding(                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m159 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96minput\u001b[0m, \u001b[96mself\u001b[0m.weight, \u001b[96mself\u001b[0m.padding_idx, \u001b[96mself\u001b[0m.max_norm,                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m160 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.norm_type, \u001b[96mself\u001b[0m.scale_grad_by_freq, \u001b[96mself\u001b[0m.sparse)                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m161 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/torch/nn/\u001b[0m\u001b[1;33mfunctional.py\u001b[0m:\u001b[94m2044\u001b[0m in      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92membedding\u001b[0m                                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2041 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m#   torch.embedding_renorm_\u001b[0m                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2042 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# remove once script supports set_grad_enabled\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2043 \u001b[0m\u001b[2m│   │   \u001b[0m_no_grad_embedding_renorm_(weight, \u001b[96minput\u001b[0m, max_norm, norm_type)                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2044 \u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m torch.embedding(weight, \u001b[96minput\u001b[0m, padding_idx, scale_grad_by_freq, sparse)        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2045 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2046 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2047 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92membedding_bag\u001b[0m(                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0mCUDA error: device-side assert triggered\n",
       "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be \n",
       "incorrect.\n",
       "For debugging consider passing \u001b[33mCUDA_LAUNCH_BLOCKING\u001b[0m=\u001b[1;36m1\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "discriminator.encoder(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f987eb24-575d-46ab-9e5f-35ad3ea26c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_output = model(past_key_values=curr_unpert_past, inputs_embeds=x,return_dict=True,output_hidden_states=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07c3f4a4-95d3-4247-9db6-7d1f740972dd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [0,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [1,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [2,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [3,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [4,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [5,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [6,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [7,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [8,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [9,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [10,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [11,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [12,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [13,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [14,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [15,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [16,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [17,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [18,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [19,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [20,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [21,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [22,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [23,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [24,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [25,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [26,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [27,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [28,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [29,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [30,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [31,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [6,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [32,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [33,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [34,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [35,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [36,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [37,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [38,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [39,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [40,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [41,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [42,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [43,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [44,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [45,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [46,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [47,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [48,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [49,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [50,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [51,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [52,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [53,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [54,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [55,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [56,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [57,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [58,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [59,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [60,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [61,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [62,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n",
      "/opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/cuda/Indexing.cu:646: indexSelectSmallIndex: block: [14,0,0], thread: [63,0,0] Assertion `srcIndex < srcSelectDimSize` failed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 output = discriminator(x)                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1102</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1099 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1100 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1101 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1102 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1103 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1104 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1105 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">32</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 29 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.cached_mode:                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 30 │   │   │   </span>avg_hidden = x.to(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.device)                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 31 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 32 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>avg_hidden = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.avg_representation(x.to(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.device))                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 33 │   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 34 │   │   </span>logits = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.classifier_head(avg_hidden)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 35 │   │   </span>probs = nn.functional.log_softmax(logits, dim=-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">avg_representation</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">23</span>                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 20 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 21 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">avg_representation</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, x):                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 22 │   │   </span>mask = x.ne(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>).unsqueeze(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>).repeat(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.embed_size).float().to(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.device   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 23 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>hidden = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.encoder.model(x)[<span style=\"color: #808000; text-decoration-color: #808000\">\"last_hidden_state\"</span>]                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 24 │   │   </span>masked_hidden = hidden * mask                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 25 │   │   </span>avg_hidden = torch.sum(masked_hidden, dim=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>) / (torch.sum(mask, dim=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>).detach()    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 26 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> avg_hidden                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1102</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1099 │   │   # this function, and just call forward.</span>                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1100 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">o</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1101 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1102 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, **kwargs)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1103 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1104 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1105 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks:                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/accelerate/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">hooks.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">165</span> in          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">new_forward</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">162 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">163 │   │   │   │   </span>output = old_forward(*args, **kwargs)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">164 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>165 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>output = old_forward(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">166 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> module._hf_hook.post_forward(module, output)                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">167 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 │   </span>module.forward = new_forward                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/transformers/models/llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">llama.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">581</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">578 │   │   │   </span>attention_mask = torch.ones(                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">579 │   │   │   │   </span>(batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embe   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">580 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>581 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>attention_mask = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._prepare_decoder_attention_mask(                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">582 │   │   │   </span>attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_len   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">583 │   │   </span>)                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">584 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/transformers/models/llama/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">llama.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">483</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_prepare_decoder_attention_mask</span>                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">480 │   │   # [bsz, seq_len] -&gt; [bsz, 1, tgt_seq_len, src_seq_len]</span>                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">481 │   │   </span>combined_attention_mask = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">482 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> input_shape[-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>] &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>:                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>483 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>combined_attention_mask = _make_causal_mask(                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">484 │   │   │   │   </span>input_shape, inputs_embeds.dtype, past_key_values_length=past_key_values   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">485 │   │   │   </span>).to(inputs_embeds.device)                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">486 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>CUDA error: device-side assert triggered\n",
       "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be \n",
       "incorrect.\n",
       "For debugging consider passing <span style=\"color: #808000; text-decoration-color: #808000\">CUDA_LAUNCH_BLOCKING</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 output = discriminator(x)                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1102\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1099 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1100 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1101 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1102 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1103 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1104 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1105 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mforward\u001b[0m:\u001b[94m32\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 29 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.cached_mode:                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 30 \u001b[0m\u001b[2m│   │   │   \u001b[0mavg_hidden = x.to(\u001b[96mself\u001b[0m.device)                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 31 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 32 \u001b[2m│   │   │   \u001b[0mavg_hidden = \u001b[96mself\u001b[0m.avg_representation(x.to(\u001b[96mself\u001b[0m.device))                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 33 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 34 \u001b[0m\u001b[2m│   │   \u001b[0mlogits = \u001b[96mself\u001b[0m.classifier_head(avg_hidden)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 35 \u001b[0m\u001b[2m│   │   \u001b[0mprobs = nn.functional.log_softmax(logits, dim=-\u001b[94m1\u001b[0m)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mavg_representation\u001b[0m:\u001b[94m23\u001b[0m                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 20 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 21 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mavg_representation\u001b[0m(\u001b[96mself\u001b[0m, x):                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 22 \u001b[0m\u001b[2m│   │   \u001b[0mmask = x.ne(\u001b[94m0\u001b[0m).unsqueeze(\u001b[94m2\u001b[0m).repeat(\u001b[94m1\u001b[0m, \u001b[94m1\u001b[0m, \u001b[96mself\u001b[0m.embed_size).float().to(\u001b[96mself\u001b[0m.device   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 23 \u001b[2m│   │   \u001b[0mhidden = \u001b[96mself\u001b[0m.encoder.model(x)[\u001b[33m\"\u001b[0m\u001b[33mlast_hidden_state\u001b[0m\u001b[33m\"\u001b[0m]                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 24 \u001b[0m\u001b[2m│   │   \u001b[0mmasked_hidden = hidden * mask                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 25 \u001b[0m\u001b[2m│   │   \u001b[0mavg_hidden = torch.sum(masked_hidden, dim=\u001b[94m1\u001b[0m) / (torch.sum(mask, dim=\u001b[94m1\u001b[0m).detach()    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 26 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m avg_hidden                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1102\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92m_call_impl\u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1099 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# this function, and just call forward.\u001b[0m                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1100 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_pre_hooks \u001b[95mo\u001b[0m  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1101 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1102 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*\u001b[96minput\u001b[0m, **kwargs)                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1103 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1104 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1105 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m _global_backward_hooks:                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/accelerate/\u001b[0m\u001b[1;33mhooks.py\u001b[0m:\u001b[94m165\u001b[0m in          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mnew_forward\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m162 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m163 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m164 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m165 \u001b[2m│   │   │   \u001b[0moutput = old_forward(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m166 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m module._hf_hook.post_forward(module, output)                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m167 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   \u001b[0mmodule.forward = new_forward                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/transformers/models/llama/\u001b[0m\u001b[1;33mmodeling_\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mllama.py\u001b[0m:\u001b[94m581\u001b[0m in \u001b[92mforward\u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m578 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask = torch.ones(                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m579 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m(batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embe   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m580 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m581 \u001b[2m│   │   \u001b[0mattention_mask = \u001b[96mself\u001b[0m._prepare_decoder_attention_mask(                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m582 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_len   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m583 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m584 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/wooseok/miniconda3/envs/mh/lib/python3.8/site-packages/transformers/models/llama/\u001b[0m\u001b[1;33mmodeling_\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[1;33mllama.py\u001b[0m:\u001b[94m483\u001b[0m in \u001b[92m_prepare_decoder_attention_mask\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m480 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\u001b[0m                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m481 \u001b[0m\u001b[2m│   │   \u001b[0mcombined_attention_mask = \u001b[94mNone\u001b[0m                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m482 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m input_shape[-\u001b[94m1\u001b[0m] > \u001b[94m1\u001b[0m:                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m483 \u001b[2m│   │   │   \u001b[0mcombined_attention_mask = _make_causal_mask(                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m484 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minput_shape, inputs_embeds.dtype, past_key_values_length=past_key_values   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m485 \u001b[0m\u001b[2m│   │   │   \u001b[0m).to(inputs_embeds.device)                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m486 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0mCUDA error: device-side assert triggered\n",
       "CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be \n",
       "incorrect.\n",
       "For debugging consider passing \u001b[33mCUDA_LAUNCH_BLOCKING\u001b[0m=\u001b[1;36m1\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = discriminator(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c49542-e4bd-4a94-ab6c-a3348f7a61cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb335004-7407-4ebe-82b5-ed1cbb03470e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a1c779b-6319-4d72-a1e2-43b6620b2cb5",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fcb079d-e68e-4fa1-98d6-9d859cd0c52b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "dd??\n",
      "Train Epoch: 1 [32/34345 (0%)]\tLoss: 1.702491\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [352/34345 (1%)]\tLoss: 1.606399\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [672/34345 (2%)]\tLoss: 1.545497\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [992/34345 (3%)]\tLoss: 1.414893\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [1312/34345 (4%)]\tLoss: 1.366233\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [1632/34345 (5%)]\tLoss: 1.315136\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [1952/34345 (6%)]\tLoss: 1.552117\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [2272/34345 (7%)]\tLoss: 1.600178\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [2592/34345 (8%)]\tLoss: 1.272506\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [2912/34345 (8%)]\tLoss: 1.265365\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [3232/34345 (9%)]\tLoss: 1.392139\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [3552/34345 (10%)]\tLoss: 1.385418\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [3872/34345 (11%)]\tLoss: 1.422530\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [4192/34345 (12%)]\tLoss: 1.374564\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [4512/34345 (13%)]\tLoss: 1.422310\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [4832/34345 (14%)]\tLoss: 1.390881\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [5152/34345 (15%)]\tLoss: 1.388985\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [5472/34345 (16%)]\tLoss: 1.203191\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [5792/34345 (17%)]\tLoss: 1.391352\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [6112/34345 (18%)]\tLoss: 1.284925\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [6432/34345 (19%)]\tLoss: 1.378020\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [6752/34345 (20%)]\tLoss: 1.362396\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [7072/34345 (21%)]\tLoss: 1.314569\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [7392/34345 (22%)]\tLoss: 1.300204\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [7712/34345 (22%)]\tLoss: 1.213301\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [8032/34345 (23%)]\tLoss: 1.208393\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [8352/34345 (24%)]\tLoss: 1.208344\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [8672/34345 (25%)]\tLoss: 1.232820\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [8992/34345 (26%)]\tLoss: 1.227879\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [9312/34345 (27%)]\tLoss: 1.309060\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [9632/34345 (28%)]\tLoss: 1.102175\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [9952/34345 (29%)]\tLoss: 1.263166\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [10272/34345 (30%)]\tLoss: 1.098550\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [10592/34345 (31%)]\tLoss: 1.108625\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [10912/34345 (32%)]\tLoss: 1.311075\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [11232/34345 (33%)]\tLoss: 1.224957\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [11552/34345 (34%)]\tLoss: 1.375355\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [11872/34345 (35%)]\tLoss: 1.203830\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [12192/34345 (35%)]\tLoss: 1.163361\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [12512/34345 (36%)]\tLoss: 1.143965\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [12832/34345 (37%)]\tLoss: 1.258518\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [13152/34345 (38%)]\tLoss: 1.305652\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [13472/34345 (39%)]\tLoss: 1.097523\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [13792/34345 (40%)]\tLoss: 1.337184\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [14112/34345 (41%)]\tLoss: 1.115869\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [14432/34345 (42%)]\tLoss: 1.088785\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [14752/34345 (43%)]\tLoss: 1.108505\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [15072/34345 (44%)]\tLoss: 1.280378\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [15392/34345 (45%)]\tLoss: 1.314744\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [15712/34345 (46%)]\tLoss: 1.256234\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [16032/34345 (47%)]\tLoss: 1.333144\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [16352/34345 (48%)]\tLoss: 1.193445\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [16672/34345 (49%)]\tLoss: 1.024319\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [16992/34345 (49%)]\tLoss: 1.041245\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [17312/34345 (50%)]\tLoss: 1.343107\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [17632/34345 (51%)]\tLoss: 1.040752\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [17952/34345 (52%)]\tLoss: 1.386888\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [18272/34345 (53%)]\tLoss: 1.282620\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [18592/34345 (54%)]\tLoss: 1.002087\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [18912/34345 (55%)]\tLoss: 1.110682\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [19232/34345 (56%)]\tLoss: 1.201790\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [19552/34345 (57%)]\tLoss: 0.979429\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [19872/34345 (58%)]\tLoss: 1.326562\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [20192/34345 (59%)]\tLoss: 1.135268\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [20512/34345 (60%)]\tLoss: 1.086840\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [20832/34345 (61%)]\tLoss: 1.181465\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [21152/34345 (62%)]\tLoss: 1.239472\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [21472/34345 (63%)]\tLoss: 1.138325\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [21792/34345 (63%)]\tLoss: 1.423673\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [22112/34345 (64%)]\tLoss: 1.042773\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [22432/34345 (65%)]\tLoss: 1.150017\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [22752/34345 (66%)]\tLoss: 1.060162\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [23072/34345 (67%)]\tLoss: 1.049498\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [23392/34345 (68%)]\tLoss: 1.233534\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [23712/34345 (69%)]\tLoss: 1.080212\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [24032/34345 (70%)]\tLoss: 1.109377\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [24352/34345 (71%)]\tLoss: 1.187811\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [24672/34345 (72%)]\tLoss: 1.151582\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [24992/34345 (73%)]\tLoss: 1.118261\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [25312/34345 (74%)]\tLoss: 1.043365\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [25632/34345 (75%)]\tLoss: 1.260963\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [25952/34345 (76%)]\tLoss: 1.008634\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [26272/34345 (76%)]\tLoss: 1.111548\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [26592/34345 (77%)]\tLoss: 1.129689\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [26912/34345 (78%)]\tLoss: 1.287437\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [27232/34345 (79%)]\tLoss: 1.038462\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [27552/34345 (80%)]\tLoss: 1.165533\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [27872/34345 (81%)]\tLoss: 1.161891\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [28192/34345 (82%)]\tLoss: 1.214665\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [28512/34345 (83%)]\tLoss: 1.157363\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [28832/34345 (84%)]\tLoss: 1.213015\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [29152/34345 (85%)]\tLoss: 1.357024\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [29472/34345 (86%)]\tLoss: 1.233649\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [29792/34345 (87%)]\tLoss: 1.088127\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [30112/34345 (88%)]\tLoss: 1.097457\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [30432/34345 (89%)]\tLoss: 1.315990\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [30752/34345 (90%)]\tLoss: 0.988965\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [31072/34345 (90%)]\tLoss: 1.138222\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [31392/34345 (91%)]\tLoss: 1.232278\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [31712/34345 (92%)]\tLoss: 1.245895\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [32032/34345 (93%)]\tLoss: 1.221945\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [32352/34345 (94%)]\tLoss: 0.969697\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [32672/34345 (95%)]\tLoss: 1.158351\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [32992/34345 (96%)]\tLoss: 1.150187\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [33312/34345 (97%)]\tLoss: 1.156494\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [33632/34345 (98%)]\tLoss: 1.332798\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [33952/34345 (99%)]\tLoss: 1.071757\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 1 [34272/34345 (100%)]\tLoss: 1.240799\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Performance on test set: Average loss: 1.1600, Accuracy: 1078/2210 (49%)\n",
      "Epoch took: 1.395s\n",
      "\n",
      "Example prediction\n",
      "dd??\n",
      "Input sentence: This is incredible! I love it, this is the best chicken I have ever had.\n",
      "Predictions: positive: 0.2590, negative: 0.0128, very positive: 0.7039, very negative: 0.0073, neutral: 0.0169\n",
      "\n",
      "Epoch 2\n",
      "dd??\n",
      "Train Epoch: 2 [32/34345 (0%)]\tLoss: 1.132375\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [352/34345 (1%)]\tLoss: 0.976829\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [672/34345 (2%)]\tLoss: 1.088350\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [992/34345 (3%)]\tLoss: 1.220885\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [1312/34345 (4%)]\tLoss: 1.066248\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [1632/34345 (5%)]\tLoss: 1.092240\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [1952/34345 (6%)]\tLoss: 1.231462\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [2272/34345 (7%)]\tLoss: 1.056624\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [2592/34345 (8%)]\tLoss: 1.195054\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [2912/34345 (8%)]\tLoss: 0.982665\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [3232/34345 (9%)]\tLoss: 1.258760\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [3552/34345 (10%)]\tLoss: 1.420915\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [3872/34345 (11%)]\tLoss: 1.104551\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [4192/34345 (12%)]\tLoss: 1.179366\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [4512/34345 (13%)]\tLoss: 1.385032\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [4832/34345 (14%)]\tLoss: 1.250569\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [5152/34345 (15%)]\tLoss: 1.105568\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [5472/34345 (16%)]\tLoss: 1.008340\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [5792/34345 (17%)]\tLoss: 1.038837\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [6112/34345 (18%)]\tLoss: 1.065450\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [6432/34345 (19%)]\tLoss: 1.030923\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [6752/34345 (20%)]\tLoss: 1.161298\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [7072/34345 (21%)]\tLoss: 1.025262\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [7392/34345 (22%)]\tLoss: 1.117265\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [7712/34345 (22%)]\tLoss: 1.175775\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [8032/34345 (23%)]\tLoss: 1.241263\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [8352/34345 (24%)]\tLoss: 1.087134\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [8672/34345 (25%)]\tLoss: 1.032961\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [8992/34345 (26%)]\tLoss: 0.942090\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [9312/34345 (27%)]\tLoss: 1.182143\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [9632/34345 (28%)]\tLoss: 1.175643\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [9952/34345 (29%)]\tLoss: 1.049910\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [10272/34345 (30%)]\tLoss: 1.133188\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [10592/34345 (31%)]\tLoss: 1.367939\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [10912/34345 (32%)]\tLoss: 1.001743\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [11232/34345 (33%)]\tLoss: 1.128626\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [11552/34345 (34%)]\tLoss: 1.135442\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [11872/34345 (35%)]\tLoss: 1.277189\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [12192/34345 (35%)]\tLoss: 1.149493\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [12512/34345 (36%)]\tLoss: 1.196693\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [12832/34345 (37%)]\tLoss: 1.150599\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [13152/34345 (38%)]\tLoss: 1.147440\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [13472/34345 (39%)]\tLoss: 1.036899\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [13792/34345 (40%)]\tLoss: 1.095827\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [14112/34345 (41%)]\tLoss: 1.056294\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [14432/34345 (42%)]\tLoss: 1.143015\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [14752/34345 (43%)]\tLoss: 1.225870\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [15072/34345 (44%)]\tLoss: 0.982445\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [15392/34345 (45%)]\tLoss: 0.977725\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [15712/34345 (46%)]\tLoss: 1.105872\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [16032/34345 (47%)]\tLoss: 1.154240\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [16352/34345 (48%)]\tLoss: 0.847258\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [16672/34345 (49%)]\tLoss: 1.119860\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [16992/34345 (49%)]\tLoss: 1.039262\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [17312/34345 (50%)]\tLoss: 0.960344\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [17632/34345 (51%)]\tLoss: 1.304069\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [17952/34345 (52%)]\tLoss: 1.029529\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [18272/34345 (53%)]\tLoss: 1.114602\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [18592/34345 (54%)]\tLoss: 0.976479\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [18912/34345 (55%)]\tLoss: 1.092684\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [19232/34345 (56%)]\tLoss: 0.830899\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [19552/34345 (57%)]\tLoss: 1.019437\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [19872/34345 (58%)]\tLoss: 0.947542\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [20192/34345 (59%)]\tLoss: 1.084456\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [20512/34345 (60%)]\tLoss: 1.127676\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [20832/34345 (61%)]\tLoss: 1.156972\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [21152/34345 (62%)]\tLoss: 0.994496\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [21472/34345 (63%)]\tLoss: 1.053477\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [21792/34345 (63%)]\tLoss: 1.190463\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [22112/34345 (64%)]\tLoss: 0.933578\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [22432/34345 (65%)]\tLoss: 1.023814\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [22752/34345 (66%)]\tLoss: 1.159788\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [23072/34345 (67%)]\tLoss: 1.096223\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [23392/34345 (68%)]\tLoss: 1.153256\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [23712/34345 (69%)]\tLoss: 1.143825\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [24032/34345 (70%)]\tLoss: 1.172861\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [24352/34345 (71%)]\tLoss: 1.347446\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [24672/34345 (72%)]\tLoss: 1.096072\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [24992/34345 (73%)]\tLoss: 1.284922\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [25312/34345 (74%)]\tLoss: 1.162370\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [25632/34345 (75%)]\tLoss: 0.950709\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [25952/34345 (76%)]\tLoss: 1.057656\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [26272/34345 (76%)]\tLoss: 1.311989\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [26592/34345 (77%)]\tLoss: 1.146251\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [26912/34345 (78%)]\tLoss: 1.133763\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [27232/34345 (79%)]\tLoss: 1.102231\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [27552/34345 (80%)]\tLoss: 1.044713\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [27872/34345 (81%)]\tLoss: 1.048152\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [28192/34345 (82%)]\tLoss: 1.018599\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [28512/34345 (83%)]\tLoss: 1.291909\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [28832/34345 (84%)]\tLoss: 1.108879\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [29152/34345 (85%)]\tLoss: 0.991069\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [29472/34345 (86%)]\tLoss: 1.033975\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [29792/34345 (87%)]\tLoss: 1.254961\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [30112/34345 (88%)]\tLoss: 1.237696\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [30432/34345 (89%)]\tLoss: 0.916627\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [30752/34345 (90%)]\tLoss: 1.014670\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [31072/34345 (90%)]\tLoss: 1.111454\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [31392/34345 (91%)]\tLoss: 1.201032\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [31712/34345 (92%)]\tLoss: 1.038017\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [32032/34345 (93%)]\tLoss: 1.106367\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [32352/34345 (94%)]\tLoss: 0.889710\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [32672/34345 (95%)]\tLoss: 1.106151\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [32992/34345 (96%)]\tLoss: 1.152290\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [33312/34345 (97%)]\tLoss: 1.080350\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [33632/34345 (98%)]\tLoss: 1.145723\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [33952/34345 (99%)]\tLoss: 1.049360\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 2 [34272/34345 (100%)]\tLoss: 0.940996\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Performance on test set: Average loss: 1.1079, Accuracy: 1112/2210 (50%)\n",
      "Epoch took: 1.280s\n",
      "\n",
      "Example prediction\n",
      "dd??\n",
      "Input sentence: This is incredible! I love it, this is the best chicken I have ever had.\n",
      "Predictions: positive: 0.1478, negative: 0.0063, very positive: 0.8352, very negative: 0.0026, neutral: 0.0080\n",
      "\n",
      "Epoch 3\n",
      "dd??\n",
      "Train Epoch: 3 [32/34345 (0%)]\tLoss: 1.076968\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [352/34345 (1%)]\tLoss: 0.852515\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [672/34345 (2%)]\tLoss: 1.140087\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [992/34345 (3%)]\tLoss: 1.192727\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [1312/34345 (4%)]\tLoss: 1.036049\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [1632/34345 (5%)]\tLoss: 1.004098\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [1952/34345 (6%)]\tLoss: 1.059280\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [2272/34345 (7%)]\tLoss: 0.920592\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [2592/34345 (8%)]\tLoss: 1.302394\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [2912/34345 (8%)]\tLoss: 0.819881\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [3232/34345 (9%)]\tLoss: 1.075808\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [3552/34345 (10%)]\tLoss: 0.964879\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [3872/34345 (11%)]\tLoss: 1.077300\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [4192/34345 (12%)]\tLoss: 0.924043\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [4512/34345 (13%)]\tLoss: 0.971218\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [4832/34345 (14%)]\tLoss: 1.076797\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [5152/34345 (15%)]\tLoss: 1.224634\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [5472/34345 (16%)]\tLoss: 1.193286\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [5792/34345 (17%)]\tLoss: 1.007453\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [6112/34345 (18%)]\tLoss: 1.104684\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [6432/34345 (19%)]\tLoss: 0.935336\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [6752/34345 (20%)]\tLoss: 0.947420\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [7072/34345 (21%)]\tLoss: 1.004053\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [7392/34345 (22%)]\tLoss: 0.939566\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [7712/34345 (22%)]\tLoss: 0.988756\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [8032/34345 (23%)]\tLoss: 1.303399\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [8352/34345 (24%)]\tLoss: 0.883682\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [8672/34345 (25%)]\tLoss: 1.240555\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [8992/34345 (26%)]\tLoss: 1.014968\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [9312/34345 (27%)]\tLoss: 0.999585\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [9632/34345 (28%)]\tLoss: 1.135903\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [9952/34345 (29%)]\tLoss: 1.119921\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [10272/34345 (30%)]\tLoss: 1.297575\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [10592/34345 (31%)]\tLoss: 1.210323\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [10912/34345 (32%)]\tLoss: 1.218198\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [11232/34345 (33%)]\tLoss: 1.048725\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [11552/34345 (34%)]\tLoss: 1.009234\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [11872/34345 (35%)]\tLoss: 1.226390\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [12192/34345 (35%)]\tLoss: 1.121319\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [12512/34345 (36%)]\tLoss: 0.990935\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [12832/34345 (37%)]\tLoss: 1.103258\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [13152/34345 (38%)]\tLoss: 0.895658\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [13472/34345 (39%)]\tLoss: 1.197489\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [13792/34345 (40%)]\tLoss: 1.398495\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [14112/34345 (41%)]\tLoss: 1.224590\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [14432/34345 (42%)]\tLoss: 1.210165\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [14752/34345 (43%)]\tLoss: 1.039207\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [15072/34345 (44%)]\tLoss: 1.084707\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [15392/34345 (45%)]\tLoss: 1.162902\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [15712/34345 (46%)]\tLoss: 0.956725\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [16032/34345 (47%)]\tLoss: 1.106881\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [16352/34345 (48%)]\tLoss: 1.060975\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [16672/34345 (49%)]\tLoss: 1.025384\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [16992/34345 (49%)]\tLoss: 1.035338\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [17312/34345 (50%)]\tLoss: 1.142144\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [17632/34345 (51%)]\tLoss: 0.956321\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [17952/34345 (52%)]\tLoss: 1.128558\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [18272/34345 (53%)]\tLoss: 1.143397\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [18592/34345 (54%)]\tLoss: 1.213329\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [18912/34345 (55%)]\tLoss: 0.847843\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [19232/34345 (56%)]\tLoss: 1.055482\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [19552/34345 (57%)]\tLoss: 0.943522\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [19872/34345 (58%)]\tLoss: 1.195673\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [20192/34345 (59%)]\tLoss: 1.182516\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [20512/34345 (60%)]\tLoss: 1.425521\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [20832/34345 (61%)]\tLoss: 0.990895\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [21152/34345 (62%)]\tLoss: 1.035319\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [21472/34345 (63%)]\tLoss: 0.954538\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [21792/34345 (63%)]\tLoss: 1.125265\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [22112/34345 (64%)]\tLoss: 0.945226\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [22432/34345 (65%)]\tLoss: 1.112703\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [22752/34345 (66%)]\tLoss: 0.908600\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [23072/34345 (67%)]\tLoss: 1.030927\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [23392/34345 (68%)]\tLoss: 1.128661\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [23712/34345 (69%)]\tLoss: 0.952193\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [24032/34345 (70%)]\tLoss: 0.974029\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [24352/34345 (71%)]\tLoss: 1.007011\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [24672/34345 (72%)]\tLoss: 1.387677\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [24992/34345 (73%)]\tLoss: 1.175792\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [25312/34345 (74%)]\tLoss: 1.186497\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [25632/34345 (75%)]\tLoss: 1.029703\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [25952/34345 (76%)]\tLoss: 0.880296\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [26272/34345 (76%)]\tLoss: 1.091568\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [26592/34345 (77%)]\tLoss: 1.201202\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [26912/34345 (78%)]\tLoss: 1.059948\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [27232/34345 (79%)]\tLoss: 1.083041\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [27552/34345 (80%)]\tLoss: 0.914865\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [27872/34345 (81%)]\tLoss: 1.167445\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [28192/34345 (82%)]\tLoss: 1.048208\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [28512/34345 (83%)]\tLoss: 1.116665\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [28832/34345 (84%)]\tLoss: 1.295818\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [29152/34345 (85%)]\tLoss: 1.113847\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [29472/34345 (86%)]\tLoss: 1.082074\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [29792/34345 (87%)]\tLoss: 1.182474\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [30112/34345 (88%)]\tLoss: 0.936540\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [30432/34345 (89%)]\tLoss: 0.964924\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [30752/34345 (90%)]\tLoss: 1.069755\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [31072/34345 (90%)]\tLoss: 1.054839\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [31392/34345 (91%)]\tLoss: 1.114865\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [31712/34345 (92%)]\tLoss: 0.921530\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [32032/34345 (93%)]\tLoss: 1.047949\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [32352/34345 (94%)]\tLoss: 0.938335\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [32672/34345 (95%)]\tLoss: 1.162938\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [32992/34345 (96%)]\tLoss: 1.008269\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [33312/34345 (97%)]\tLoss: 1.112575\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [33632/34345 (98%)]\tLoss: 1.181601\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [33952/34345 (99%)]\tLoss: 0.969973\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 3 [34272/34345 (100%)]\tLoss: 0.829515\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Performance on test set: Average loss: 1.1102, Accuracy: 1113/2210 (50%)\n",
      "Epoch took: 1.326s\n",
      "\n",
      "Example prediction\n",
      "dd??\n",
      "Input sentence: This is incredible! I love it, this is the best chicken I have ever had.\n",
      "Predictions: positive: 0.1834, negative: 0.0034, very positive: 0.8071, very negative: 0.0015, neutral: 0.0046\n",
      "\n",
      "Epoch 4\n",
      "dd??\n",
      "Train Epoch: 4 [32/34345 (0%)]\tLoss: 1.045195\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [352/34345 (1%)]\tLoss: 0.987362\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [672/34345 (2%)]\tLoss: 1.246616\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [992/34345 (3%)]\tLoss: 1.043168\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [1312/34345 (4%)]\tLoss: 0.958684\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [1632/34345 (5%)]\tLoss: 1.114824\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [1952/34345 (6%)]\tLoss: 1.194743\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [2272/34345 (7%)]\tLoss: 0.889106\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [2592/34345 (8%)]\tLoss: 0.984857\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [2912/34345 (8%)]\tLoss: 1.131872\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [3232/34345 (9%)]\tLoss: 1.046990\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [3552/34345 (10%)]\tLoss: 1.171448\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [3872/34345 (11%)]\tLoss: 1.061100\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [4192/34345 (12%)]\tLoss: 1.048569\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [4512/34345 (13%)]\tLoss: 1.034398\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [4832/34345 (14%)]\tLoss: 0.748345\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [5152/34345 (15%)]\tLoss: 1.282391\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [5472/34345 (16%)]\tLoss: 1.000515\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [5792/34345 (17%)]\tLoss: 1.043443\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [6112/34345 (18%)]\tLoss: 0.950060\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [6432/34345 (19%)]\tLoss: 0.907764\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [6752/34345 (20%)]\tLoss: 1.012808\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [7072/34345 (21%)]\tLoss: 0.794944\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [7392/34345 (22%)]\tLoss: 1.078724\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [7712/34345 (22%)]\tLoss: 0.903486\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [8032/34345 (23%)]\tLoss: 1.283855\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [8352/34345 (24%)]\tLoss: 0.979539\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [8672/34345 (25%)]\tLoss: 0.880586\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [8992/34345 (26%)]\tLoss: 1.216910\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [9312/34345 (27%)]\tLoss: 0.956528\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [9632/34345 (28%)]\tLoss: 0.831217\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [9952/34345 (29%)]\tLoss: 0.982482\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [10272/34345 (30%)]\tLoss: 1.061582\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [10592/34345 (31%)]\tLoss: 0.948253\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [10912/34345 (32%)]\tLoss: 1.274376\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [11232/34345 (33%)]\tLoss: 0.897651\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [11552/34345 (34%)]\tLoss: 0.904789\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [11872/34345 (35%)]\tLoss: 1.049391\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [12192/34345 (35%)]\tLoss: 0.983503\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [12512/34345 (36%)]\tLoss: 1.136462\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [12832/34345 (37%)]\tLoss: 1.019840\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [13152/34345 (38%)]\tLoss: 0.982859\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [13472/34345 (39%)]\tLoss: 0.835427\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [13792/34345 (40%)]\tLoss: 1.216034\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [14112/34345 (41%)]\tLoss: 1.125283\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [14432/34345 (42%)]\tLoss: 1.033016\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [14752/34345 (43%)]\tLoss: 0.926186\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [15072/34345 (44%)]\tLoss: 0.916903\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [15392/34345 (45%)]\tLoss: 1.075123\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [15712/34345 (46%)]\tLoss: 1.053330\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [16032/34345 (47%)]\tLoss: 1.098488\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [16352/34345 (48%)]\tLoss: 1.018763\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [16672/34345 (49%)]\tLoss: 1.222729\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [16992/34345 (49%)]\tLoss: 0.716827\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [17312/34345 (50%)]\tLoss: 1.047933\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [17632/34345 (51%)]\tLoss: 1.290714\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [17952/34345 (52%)]\tLoss: 1.142755\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [18272/34345 (53%)]\tLoss: 1.138670\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [18592/34345 (54%)]\tLoss: 1.073434\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [18912/34345 (55%)]\tLoss: 1.019286\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [19232/34345 (56%)]\tLoss: 1.080711\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [19552/34345 (57%)]\tLoss: 0.787565\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [19872/34345 (58%)]\tLoss: 0.906131\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [20192/34345 (59%)]\tLoss: 1.173268\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [20512/34345 (60%)]\tLoss: 1.193401\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [20832/34345 (61%)]\tLoss: 1.024945\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [21152/34345 (62%)]\tLoss: 1.083960\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [21472/34345 (63%)]\tLoss: 0.999530\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [21792/34345 (63%)]\tLoss: 1.011776\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [22112/34345 (64%)]\tLoss: 1.270821\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [22432/34345 (65%)]\tLoss: 0.953122\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [22752/34345 (66%)]\tLoss: 1.203901\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [23072/34345 (67%)]\tLoss: 0.830095\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [23392/34345 (68%)]\tLoss: 0.978780\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [23712/34345 (69%)]\tLoss: 0.956659\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [24032/34345 (70%)]\tLoss: 1.405394\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [24352/34345 (71%)]\tLoss: 1.145309\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [24672/34345 (72%)]\tLoss: 1.239447\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [24992/34345 (73%)]\tLoss: 1.010633\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [25312/34345 (74%)]\tLoss: 1.008174\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [25632/34345 (75%)]\tLoss: 1.022867\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [25952/34345 (76%)]\tLoss: 0.851840\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [26272/34345 (76%)]\tLoss: 0.952102\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [26592/34345 (77%)]\tLoss: 0.946758\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [26912/34345 (78%)]\tLoss: 1.189208\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [27232/34345 (79%)]\tLoss: 0.988172\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [27552/34345 (80%)]\tLoss: 0.970208\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [27872/34345 (81%)]\tLoss: 0.947952\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [28192/34345 (82%)]\tLoss: 1.017727\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [28512/34345 (83%)]\tLoss: 1.132099\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [28832/34345 (84%)]\tLoss: 0.932649\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [29152/34345 (85%)]\tLoss: 0.990550\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [29472/34345 (86%)]\tLoss: 1.100467\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [29792/34345 (87%)]\tLoss: 1.136608\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [30112/34345 (88%)]\tLoss: 0.881121\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [30432/34345 (89%)]\tLoss: 1.034684\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [30752/34345 (90%)]\tLoss: 1.020201\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [31072/34345 (90%)]\tLoss: 1.038731\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [31392/34345 (91%)]\tLoss: 1.042708\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [31712/34345 (92%)]\tLoss: 1.368274\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [32032/34345 (93%)]\tLoss: 0.922857\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [32352/34345 (94%)]\tLoss: 1.274372\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [32672/34345 (95%)]\tLoss: 1.063537\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [32992/34345 (96%)]\tLoss: 1.042865\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [33312/34345 (97%)]\tLoss: 1.022065\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [33632/34345 (98%)]\tLoss: 1.067564\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [33952/34345 (99%)]\tLoss: 0.994467\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 4 [34272/34345 (100%)]\tLoss: 0.921573\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Performance on test set: Average loss: 1.0739, Accuracy: 1151/2210 (52%)\n",
      "Epoch took: 1.315s\n",
      "\n",
      "Example prediction\n",
      "dd??\n",
      "Input sentence: This is incredible! I love it, this is the best chicken I have ever had.\n",
      "Predictions: positive: 0.1002, negative: 0.0012, very positive: 0.8957, very negative: 0.0007, neutral: 0.0022\n",
      "\n",
      "Epoch 5\n",
      "dd??\n",
      "Train Epoch: 5 [32/34345 (0%)]\tLoss: 0.926129\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [352/34345 (1%)]\tLoss: 1.036467\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [672/34345 (2%)]\tLoss: 1.028817\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [992/34345 (3%)]\tLoss: 1.073752\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [1312/34345 (4%)]\tLoss: 1.136119\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [1632/34345 (5%)]\tLoss: 1.026356\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [1952/34345 (6%)]\tLoss: 1.069327\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [2272/34345 (7%)]\tLoss: 0.990954\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [2592/34345 (8%)]\tLoss: 1.109041\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [2912/34345 (8%)]\tLoss: 1.025041\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [3232/34345 (9%)]\tLoss: 1.012624\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [3552/34345 (10%)]\tLoss: 1.216086\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [3872/34345 (11%)]\tLoss: 1.238159\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [4192/34345 (12%)]\tLoss: 1.131746\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [4512/34345 (13%)]\tLoss: 0.791751\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [4832/34345 (14%)]\tLoss: 0.927982\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [5152/34345 (15%)]\tLoss: 1.001923\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [5472/34345 (16%)]\tLoss: 0.924598\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [5792/34345 (17%)]\tLoss: 1.098046\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [6112/34345 (18%)]\tLoss: 1.005182\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [6432/34345 (19%)]\tLoss: 0.963366\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [6752/34345 (20%)]\tLoss: 1.001284\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [7072/34345 (21%)]\tLoss: 0.931804\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [7392/34345 (22%)]\tLoss: 1.035981\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [7712/34345 (22%)]\tLoss: 0.974047\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [8032/34345 (23%)]\tLoss: 0.970159\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [8352/34345 (24%)]\tLoss: 1.017806\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [8672/34345 (25%)]\tLoss: 1.104635\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [8992/34345 (26%)]\tLoss: 0.913660\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [9312/34345 (27%)]\tLoss: 1.134923\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [9632/34345 (28%)]\tLoss: 1.143234\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [9952/34345 (29%)]\tLoss: 0.918022\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [10272/34345 (30%)]\tLoss: 1.003482\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [10592/34345 (31%)]\tLoss: 1.033006\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [10912/34345 (32%)]\tLoss: 1.027025\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [11232/34345 (33%)]\tLoss: 1.217059\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [11552/34345 (34%)]\tLoss: 1.078411\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [11872/34345 (35%)]\tLoss: 1.009496\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [12192/34345 (35%)]\tLoss: 1.019217\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [12512/34345 (36%)]\tLoss: 0.929335\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [12832/34345 (37%)]\tLoss: 0.884660\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [13152/34345 (38%)]\tLoss: 1.092394\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [13472/34345 (39%)]\tLoss: 0.973144\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [13792/34345 (40%)]\tLoss: 0.940222\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [14112/34345 (41%)]\tLoss: 1.012537\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [14432/34345 (42%)]\tLoss: 0.785602\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [14752/34345 (43%)]\tLoss: 0.988817\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [15072/34345 (44%)]\tLoss: 1.026939\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [15392/34345 (45%)]\tLoss: 1.072243\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [15712/34345 (46%)]\tLoss: 0.871594\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [16032/34345 (47%)]\tLoss: 0.891727\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [16352/34345 (48%)]\tLoss: 1.016044\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [16672/34345 (49%)]\tLoss: 0.931391\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [16992/34345 (49%)]\tLoss: 0.982338\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [17312/34345 (50%)]\tLoss: 1.045618\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [17632/34345 (51%)]\tLoss: 0.764657\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [17952/34345 (52%)]\tLoss: 1.114787\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [18272/34345 (53%)]\tLoss: 0.914401\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [18592/34345 (54%)]\tLoss: 0.879418\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [18912/34345 (55%)]\tLoss: 1.035345\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [19232/34345 (56%)]\tLoss: 1.012138\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [19552/34345 (57%)]\tLoss: 0.836173\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [19872/34345 (58%)]\tLoss: 1.199019\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [20192/34345 (59%)]\tLoss: 0.887909\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [20512/34345 (60%)]\tLoss: 0.876913\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [20832/34345 (61%)]\tLoss: 1.184868\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [21152/34345 (62%)]\tLoss: 0.989170\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [21472/34345 (63%)]\tLoss: 0.974764\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [21792/34345 (63%)]\tLoss: 0.878559\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [22112/34345 (64%)]\tLoss: 1.020367\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [22432/34345 (65%)]\tLoss: 1.014932\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [22752/34345 (66%)]\tLoss: 1.057313\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [23072/34345 (67%)]\tLoss: 1.048583\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [23392/34345 (68%)]\tLoss: 0.859664\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [23712/34345 (69%)]\tLoss: 0.937516\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [24032/34345 (70%)]\tLoss: 1.026678\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [24352/34345 (71%)]\tLoss: 1.035558\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [24672/34345 (72%)]\tLoss: 1.065820\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [24992/34345 (73%)]\tLoss: 1.076748\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [25312/34345 (74%)]\tLoss: 1.109105\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [25632/34345 (75%)]\tLoss: 0.986696\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [25952/34345 (76%)]\tLoss: 0.907955\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [26272/34345 (76%)]\tLoss: 0.984879\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [26592/34345 (77%)]\tLoss: 0.913155\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [26912/34345 (78%)]\tLoss: 0.993576\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [27232/34345 (79%)]\tLoss: 1.208894\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [27552/34345 (80%)]\tLoss: 1.328390\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [27872/34345 (81%)]\tLoss: 0.919175\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [28192/34345 (82%)]\tLoss: 0.887724\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [28512/34345 (83%)]\tLoss: 0.938451\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [28832/34345 (84%)]\tLoss: 1.073941\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [29152/34345 (85%)]\tLoss: 1.010383\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [29472/34345 (86%)]\tLoss: 1.080447\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [29792/34345 (87%)]\tLoss: 1.152345\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [30112/34345 (88%)]\tLoss: 0.788534\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [30432/34345 (89%)]\tLoss: 1.065214\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [30752/34345 (90%)]\tLoss: 1.110369\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [31072/34345 (90%)]\tLoss: 1.063514\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [31392/34345 (91%)]\tLoss: 1.056741\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [31712/34345 (92%)]\tLoss: 0.980737\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [32032/34345 (93%)]\tLoss: 0.897296\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [32352/34345 (94%)]\tLoss: 1.002737\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [32672/34345 (95%)]\tLoss: 0.965624\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [32992/34345 (96%)]\tLoss: 1.220888\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [33312/34345 (97%)]\tLoss: 1.247420\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [33632/34345 (98%)]\tLoss: 0.721044\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [33952/34345 (99%)]\tLoss: 1.123079\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 5 [34272/34345 (100%)]\tLoss: 1.178739\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Performance on test set: Average loss: 1.0782, Accuracy: 1135/2210 (51%)\n",
      "Epoch took: 1.316s\n",
      "\n",
      "Example prediction\n",
      "dd??\n",
      "Input sentence: This is incredible! I love it, this is the best chicken I have ever had.\n",
      "Predictions: positive: 0.1340, negative: 0.0014, very positive: 0.8617, very negative: 0.0013, neutral: 0.0016\n",
      "\n",
      "Epoch 6\n",
      "dd??\n",
      "Train Epoch: 6 [32/34345 (0%)]\tLoss: 1.072300\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [352/34345 (1%)]\tLoss: 1.104203\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [672/34345 (2%)]\tLoss: 1.005870\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [992/34345 (3%)]\tLoss: 0.901304\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [1312/34345 (4%)]\tLoss: 0.905056\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [1632/34345 (5%)]\tLoss: 0.938793\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [1952/34345 (6%)]\tLoss: 1.168001\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [2272/34345 (7%)]\tLoss: 1.011115\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [2592/34345 (8%)]\tLoss: 0.857024\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [2912/34345 (8%)]\tLoss: 0.978014\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [3232/34345 (9%)]\tLoss: 0.850113\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [3552/34345 (10%)]\tLoss: 1.064231\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [3872/34345 (11%)]\tLoss: 1.029893\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [4192/34345 (12%)]\tLoss: 0.950995\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [4512/34345 (13%)]\tLoss: 1.008033\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [4832/34345 (14%)]\tLoss: 1.025125\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [5152/34345 (15%)]\tLoss: 0.929350\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [5472/34345 (16%)]\tLoss: 1.162311\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [5792/34345 (17%)]\tLoss: 0.881443\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [6112/34345 (18%)]\tLoss: 0.973749\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [6432/34345 (19%)]\tLoss: 1.054364\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [6752/34345 (20%)]\tLoss: 1.029263\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [7072/34345 (21%)]\tLoss: 1.161774\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [7392/34345 (22%)]\tLoss: 0.830145\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [7712/34345 (22%)]\tLoss: 0.944340\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [8032/34345 (23%)]\tLoss: 1.107623\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [8352/34345 (24%)]\tLoss: 1.053781\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [8672/34345 (25%)]\tLoss: 1.241994\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [8992/34345 (26%)]\tLoss: 0.842230\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [9312/34345 (27%)]\tLoss: 0.908757\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [9632/34345 (28%)]\tLoss: 0.820202\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [9952/34345 (29%)]\tLoss: 1.036435\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [10272/34345 (30%)]\tLoss: 1.161350\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [10592/34345 (31%)]\tLoss: 1.134990\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [10912/34345 (32%)]\tLoss: 1.101759\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [11232/34345 (33%)]\tLoss: 1.067271\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [11552/34345 (34%)]\tLoss: 1.044138\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [11872/34345 (35%)]\tLoss: 0.911314\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [12192/34345 (35%)]\tLoss: 1.170942\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [12512/34345 (36%)]\tLoss: 0.945364\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [12832/34345 (37%)]\tLoss: 0.939116\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [13152/34345 (38%)]\tLoss: 1.181839\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [13472/34345 (39%)]\tLoss: 0.963901\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [13792/34345 (40%)]\tLoss: 0.963189\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [14112/34345 (41%)]\tLoss: 1.203693\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [14432/34345 (42%)]\tLoss: 0.823313\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [14752/34345 (43%)]\tLoss: 0.991711\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [15072/34345 (44%)]\tLoss: 1.101614\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [15392/34345 (45%)]\tLoss: 0.919894\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [15712/34345 (46%)]\tLoss: 0.894171\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [16032/34345 (47%)]\tLoss: 0.920577\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [16352/34345 (48%)]\tLoss: 1.082034\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [16672/34345 (49%)]\tLoss: 1.082035\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [16992/34345 (49%)]\tLoss: 1.000347\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [17312/34345 (50%)]\tLoss: 1.019428\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [17632/34345 (51%)]\tLoss: 1.005284\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [17952/34345 (52%)]\tLoss: 1.210398\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [18272/34345 (53%)]\tLoss: 0.967810\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [18592/34345 (54%)]\tLoss: 0.907100\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [18912/34345 (55%)]\tLoss: 1.288156\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [19232/34345 (56%)]\tLoss: 0.907063\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [19552/34345 (57%)]\tLoss: 1.136782\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [19872/34345 (58%)]\tLoss: 0.969051\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [20192/34345 (59%)]\tLoss: 0.846844\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [20512/34345 (60%)]\tLoss: 1.130446\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [20832/34345 (61%)]\tLoss: 0.949595\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [21152/34345 (62%)]\tLoss: 1.024690\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [21472/34345 (63%)]\tLoss: 1.049188\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [21792/34345 (63%)]\tLoss: 1.000175\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [22112/34345 (64%)]\tLoss: 0.968291\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [22432/34345 (65%)]\tLoss: 1.138114\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [22752/34345 (66%)]\tLoss: 0.841522\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [23072/34345 (67%)]\tLoss: 0.919450\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [23392/34345 (68%)]\tLoss: 0.950350\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [23712/34345 (69%)]\tLoss: 0.893692\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [24032/34345 (70%)]\tLoss: 0.965272\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [24352/34345 (71%)]\tLoss: 0.906877\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [24672/34345 (72%)]\tLoss: 0.986964\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [24992/34345 (73%)]\tLoss: 1.031325\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [25312/34345 (74%)]\tLoss: 1.130680\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [25632/34345 (75%)]\tLoss: 0.950232\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [25952/34345 (76%)]\tLoss: 1.008341\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [26272/34345 (76%)]\tLoss: 0.926609\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [26592/34345 (77%)]\tLoss: 0.970299\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [26912/34345 (78%)]\tLoss: 0.846489\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [27232/34345 (79%)]\tLoss: 0.948176\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [27552/34345 (80%)]\tLoss: 0.934376\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [27872/34345 (81%)]\tLoss: 1.055029\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [28192/34345 (82%)]\tLoss: 0.961722\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [28512/34345 (83%)]\tLoss: 1.001662\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [28832/34345 (84%)]\tLoss: 1.054737\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [29152/34345 (85%)]\tLoss: 1.077387\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [29472/34345 (86%)]\tLoss: 0.964206\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [29792/34345 (87%)]\tLoss: 1.064069\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [30112/34345 (88%)]\tLoss: 0.947598\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [30432/34345 (89%)]\tLoss: 0.838485\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [30752/34345 (90%)]\tLoss: 0.948149\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [31072/34345 (90%)]\tLoss: 1.093308\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [31392/34345 (91%)]\tLoss: 1.074553\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [31712/34345 (92%)]\tLoss: 0.936344\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [32032/34345 (93%)]\tLoss: 1.047772\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [32352/34345 (94%)]\tLoss: 1.214208\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [32672/34345 (95%)]\tLoss: 1.188070\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [32992/34345 (96%)]\tLoss: 1.128650\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [33312/34345 (97%)]\tLoss: 0.886005\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [33632/34345 (98%)]\tLoss: 0.950724\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [33952/34345 (99%)]\tLoss: 1.090324\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 6 [34272/34345 (100%)]\tLoss: 1.096160\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Performance on test set: Average loss: 1.0594, Accuracy: 1155/2210 (52%)\n",
      "Epoch took: 1.335s\n",
      "\n",
      "Example prediction\n",
      "dd??\n",
      "Input sentence: This is incredible! I love it, this is the best chicken I have ever had.\n",
      "Predictions: positive: 0.0432, negative: 0.0004, very positive: 0.9556, very negative: 0.0002, neutral: 0.0006\n",
      "\n",
      "Epoch 7\n",
      "dd??\n",
      "Train Epoch: 7 [32/34345 (0%)]\tLoss: 0.976854\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [352/34345 (1%)]\tLoss: 0.974661\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [672/34345 (2%)]\tLoss: 0.897997\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [992/34345 (3%)]\tLoss: 1.025716\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [1312/34345 (4%)]\tLoss: 0.892531\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [1632/34345 (5%)]\tLoss: 1.052705\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [1952/34345 (6%)]\tLoss: 0.984195\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [2272/34345 (7%)]\tLoss: 1.065489\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [2592/34345 (8%)]\tLoss: 0.765320\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [2912/34345 (8%)]\tLoss: 1.096554\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [3232/34345 (9%)]\tLoss: 1.038546\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [3552/34345 (10%)]\tLoss: 0.980339\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [3872/34345 (11%)]\tLoss: 0.930456\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [4192/34345 (12%)]\tLoss: 0.950392\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [4512/34345 (13%)]\tLoss: 0.965959\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [4832/34345 (14%)]\tLoss: 1.108799\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [5152/34345 (15%)]\tLoss: 0.771399\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [5472/34345 (16%)]\tLoss: 1.096998\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [5792/34345 (17%)]\tLoss: 1.064281\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [6112/34345 (18%)]\tLoss: 1.337967\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [6432/34345 (19%)]\tLoss: 1.120956\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [6752/34345 (20%)]\tLoss: 1.059965\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [7072/34345 (21%)]\tLoss: 0.909468\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [7392/34345 (22%)]\tLoss: 1.172348\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [7712/34345 (22%)]\tLoss: 1.024822\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [8032/34345 (23%)]\tLoss: 1.206941\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [8352/34345 (24%)]\tLoss: 0.914923\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [8672/34345 (25%)]\tLoss: 0.890244\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [8992/34345 (26%)]\tLoss: 1.110330\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [9312/34345 (27%)]\tLoss: 0.989131\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [9632/34345 (28%)]\tLoss: 0.981160\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [9952/34345 (29%)]\tLoss: 0.954795\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [10272/34345 (30%)]\tLoss: 0.899041\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [10592/34345 (31%)]\tLoss: 1.012525\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [10912/34345 (32%)]\tLoss: 1.090382\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [11232/34345 (33%)]\tLoss: 0.827693\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [11552/34345 (34%)]\tLoss: 0.976609\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [11872/34345 (35%)]\tLoss: 0.930111\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [12192/34345 (35%)]\tLoss: 1.016293\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [12512/34345 (36%)]\tLoss: 0.720736\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [12832/34345 (37%)]\tLoss: 0.825171\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [13152/34345 (38%)]\tLoss: 1.057489\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [13472/34345 (39%)]\tLoss: 0.981324\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [13792/34345 (40%)]\tLoss: 1.015953\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [14112/34345 (41%)]\tLoss: 0.878953\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [14432/34345 (42%)]\tLoss: 0.841140\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [14752/34345 (43%)]\tLoss: 1.005576\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [15072/34345 (44%)]\tLoss: 1.109813\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [15392/34345 (45%)]\tLoss: 0.980237\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [15712/34345 (46%)]\tLoss: 1.084288\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [16032/34345 (47%)]\tLoss: 1.019735\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [16352/34345 (48%)]\tLoss: 1.016216\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [16672/34345 (49%)]\tLoss: 1.011868\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [16992/34345 (49%)]\tLoss: 0.915700\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [17312/34345 (50%)]\tLoss: 1.016850\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [17632/34345 (51%)]\tLoss: 1.002970\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [17952/34345 (52%)]\tLoss: 1.004395\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [18272/34345 (53%)]\tLoss: 1.069319\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [18592/34345 (54%)]\tLoss: 1.123899\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [18912/34345 (55%)]\tLoss: 0.898026\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [19232/34345 (56%)]\tLoss: 0.745206\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [19552/34345 (57%)]\tLoss: 0.760756\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [19872/34345 (58%)]\tLoss: 0.939089\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [20192/34345 (59%)]\tLoss: 1.090898\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [20512/34345 (60%)]\tLoss: 0.916880\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [20832/34345 (61%)]\tLoss: 0.914878\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [21152/34345 (62%)]\tLoss: 1.019537\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [21472/34345 (63%)]\tLoss: 0.957319\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [21792/34345 (63%)]\tLoss: 0.942779\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [22112/34345 (64%)]\tLoss: 0.996030\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [22432/34345 (65%)]\tLoss: 0.779261\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [22752/34345 (66%)]\tLoss: 1.067394\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [23072/34345 (67%)]\tLoss: 0.810331\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [23392/34345 (68%)]\tLoss: 0.986028\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [23712/34345 (69%)]\tLoss: 1.006957\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [24032/34345 (70%)]\tLoss: 0.992636\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [24352/34345 (71%)]\tLoss: 1.034025\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [24672/34345 (72%)]\tLoss: 0.852467\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [24992/34345 (73%)]\tLoss: 0.980210\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [25312/34345 (74%)]\tLoss: 0.827531\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [25632/34345 (75%)]\tLoss: 1.057527\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [25952/34345 (76%)]\tLoss: 1.177142\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [26272/34345 (76%)]\tLoss: 1.194553\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [26592/34345 (77%)]\tLoss: 0.956772\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [26912/34345 (78%)]\tLoss: 1.021004\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [27232/34345 (79%)]\tLoss: 0.942878\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [27552/34345 (80%)]\tLoss: 0.920187\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [27872/34345 (81%)]\tLoss: 1.287962\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [28192/34345 (82%)]\tLoss: 0.835168\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [28512/34345 (83%)]\tLoss: 0.868830\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [28832/34345 (84%)]\tLoss: 1.013793\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [29152/34345 (85%)]\tLoss: 1.011057\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [29472/34345 (86%)]\tLoss: 0.903775\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [29792/34345 (87%)]\tLoss: 0.806078\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [30112/34345 (88%)]\tLoss: 0.863130\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [30432/34345 (89%)]\tLoss: 1.048608\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [30752/34345 (90%)]\tLoss: 1.158289\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [31072/34345 (90%)]\tLoss: 0.951917\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [31392/34345 (91%)]\tLoss: 0.785773\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [31712/34345 (92%)]\tLoss: 1.001694\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [32032/34345 (93%)]\tLoss: 0.978349\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [32352/34345 (94%)]\tLoss: 1.098148\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [32672/34345 (95%)]\tLoss: 1.023548\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [32992/34345 (96%)]\tLoss: 0.885755\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [33312/34345 (97%)]\tLoss: 0.873088\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [33632/34345 (98%)]\tLoss: 0.991580\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [33952/34345 (99%)]\tLoss: 0.879818\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 7 [34272/34345 (100%)]\tLoss: 1.003637\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Performance on test set: Average loss: 1.0983, Accuracy: 1080/2210 (49%)\n",
      "Epoch took: 1.318s\n",
      "\n",
      "Example prediction\n",
      "dd??\n",
      "Input sentence: This is incredible! I love it, this is the best chicken I have ever had.\n",
      "Predictions: positive: 0.1133, negative: 0.0005, very positive: 0.8839, very negative: 0.0005, neutral: 0.0018\n",
      "\n",
      "Epoch 8\n",
      "dd??\n",
      "Train Epoch: 8 [32/34345 (0%)]\tLoss: 1.209247\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [352/34345 (1%)]\tLoss: 0.852013\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [672/34345 (2%)]\tLoss: 1.032737\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [992/34345 (3%)]\tLoss: 1.030549\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [1312/34345 (4%)]\tLoss: 0.841525\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [1632/34345 (5%)]\tLoss: 1.189715\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [1952/34345 (6%)]\tLoss: 1.110691\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [2272/34345 (7%)]\tLoss: 1.076516\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [2592/34345 (8%)]\tLoss: 1.078114\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [2912/34345 (8%)]\tLoss: 1.006085\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [3232/34345 (9%)]\tLoss: 1.094795\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [3552/34345 (10%)]\tLoss: 0.979512\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [3872/34345 (11%)]\tLoss: 0.945857\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [4192/34345 (12%)]\tLoss: 1.141960\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [4512/34345 (13%)]\tLoss: 0.954175\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [4832/34345 (14%)]\tLoss: 1.092992\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [5152/34345 (15%)]\tLoss: 0.910778\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [5472/34345 (16%)]\tLoss: 1.173737\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [5792/34345 (17%)]\tLoss: 0.894520\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [6112/34345 (18%)]\tLoss: 0.911911\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [6432/34345 (19%)]\tLoss: 1.031238\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [6752/34345 (20%)]\tLoss: 1.121599\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [7072/34345 (21%)]\tLoss: 1.156471\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [7392/34345 (22%)]\tLoss: 0.938286\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [7712/34345 (22%)]\tLoss: 0.958467\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [8032/34345 (23%)]\tLoss: 0.899214\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [8352/34345 (24%)]\tLoss: 1.019675\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [8672/34345 (25%)]\tLoss: 1.029357\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [8992/34345 (26%)]\tLoss: 1.009165\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [9312/34345 (27%)]\tLoss: 0.999001\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [9632/34345 (28%)]\tLoss: 0.937164\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [9952/34345 (29%)]\tLoss: 0.928527\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [10272/34345 (30%)]\tLoss: 0.818850\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [10592/34345 (31%)]\tLoss: 1.138042\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [10912/34345 (32%)]\tLoss: 0.877017\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [11232/34345 (33%)]\tLoss: 1.096058\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [11552/34345 (34%)]\tLoss: 1.146020\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [11872/34345 (35%)]\tLoss: 0.954077\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [12192/34345 (35%)]\tLoss: 0.943161\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [12512/34345 (36%)]\tLoss: 1.019944\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [12832/34345 (37%)]\tLoss: 1.027331\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [13152/34345 (38%)]\tLoss: 1.164711\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [13472/34345 (39%)]\tLoss: 0.972898\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [13792/34345 (40%)]\tLoss: 0.869356\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [14112/34345 (41%)]\tLoss: 0.893821\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [14432/34345 (42%)]\tLoss: 1.026366\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [14752/34345 (43%)]\tLoss: 1.216357\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [15072/34345 (44%)]\tLoss: 0.980103\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [15392/34345 (45%)]\tLoss: 1.030247\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [15712/34345 (46%)]\tLoss: 1.069080\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [16032/34345 (47%)]\tLoss: 1.090936\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [16352/34345 (48%)]\tLoss: 0.859157\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [16672/34345 (49%)]\tLoss: 0.886389\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [16992/34345 (49%)]\tLoss: 1.074064\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [17312/34345 (50%)]\tLoss: 1.142738\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [17632/34345 (51%)]\tLoss: 0.887975\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [17952/34345 (52%)]\tLoss: 1.037641\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [18272/34345 (53%)]\tLoss: 0.665050\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [18592/34345 (54%)]\tLoss: 1.009514\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [18912/34345 (55%)]\tLoss: 0.926601\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [19232/34345 (56%)]\tLoss: 0.890100\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [19552/34345 (57%)]\tLoss: 1.009969\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [19872/34345 (58%)]\tLoss: 0.910504\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [20192/34345 (59%)]\tLoss: 0.854447\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [20512/34345 (60%)]\tLoss: 1.150543\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [20832/34345 (61%)]\tLoss: 1.029821\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [21152/34345 (62%)]\tLoss: 0.874953\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [21472/34345 (63%)]\tLoss: 1.119827\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [21792/34345 (63%)]\tLoss: 0.891346\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [22112/34345 (64%)]\tLoss: 1.026446\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [22432/34345 (65%)]\tLoss: 0.851329\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [22752/34345 (66%)]\tLoss: 1.200079\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [23072/34345 (67%)]\tLoss: 1.223952\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [23392/34345 (68%)]\tLoss: 1.248697\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [23712/34345 (69%)]\tLoss: 0.942412\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [24032/34345 (70%)]\tLoss: 0.956923\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [24352/34345 (71%)]\tLoss: 1.088170\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [24672/34345 (72%)]\tLoss: 1.015989\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [24992/34345 (73%)]\tLoss: 0.993113\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [25312/34345 (74%)]\tLoss: 0.946667\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [25632/34345 (75%)]\tLoss: 0.943539\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [25952/34345 (76%)]\tLoss: 0.987599\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [26272/34345 (76%)]\tLoss: 1.139984\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [26592/34345 (77%)]\tLoss: 0.845319\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [26912/34345 (78%)]\tLoss: 1.167646\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [27232/34345 (79%)]\tLoss: 0.760038\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [27552/34345 (80%)]\tLoss: 0.933451\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [27872/34345 (81%)]\tLoss: 0.817100\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [28192/34345 (82%)]\tLoss: 1.112538\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [28512/34345 (83%)]\tLoss: 1.083448\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [28832/34345 (84%)]\tLoss: 0.968693\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [29152/34345 (85%)]\tLoss: 1.376294\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [29472/34345 (86%)]\tLoss: 0.889874\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [29792/34345 (87%)]\tLoss: 0.779353\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [30112/34345 (88%)]\tLoss: 0.857159\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [30432/34345 (89%)]\tLoss: 0.923186\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [30752/34345 (90%)]\tLoss: 0.928135\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [31072/34345 (90%)]\tLoss: 0.862017\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [31392/34345 (91%)]\tLoss: 0.984798\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [31712/34345 (92%)]\tLoss: 1.272636\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [32032/34345 (93%)]\tLoss: 1.168160\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [32352/34345 (94%)]\tLoss: 1.024613\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [32672/34345 (95%)]\tLoss: 0.919819\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [32992/34345 (96%)]\tLoss: 0.941986\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [33312/34345 (97%)]\tLoss: 1.121235\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [33632/34345 (98%)]\tLoss: 0.931031\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [33952/34345 (99%)]\tLoss: 1.085044\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 8 [34272/34345 (100%)]\tLoss: 1.169264\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Performance on test set: Average loss: 1.0431, Accuracy: 1212/2210 (55%)\n",
      "Epoch took: 1.330s\n",
      "\n",
      "Example prediction\n",
      "dd??\n",
      "Input sentence: This is incredible! I love it, this is the best chicken I have ever had.\n",
      "Predictions: positive: 0.0672, negative: 0.0004, very positive: 0.9317, very negative: 0.0003, neutral: 0.0004\n",
      "\n",
      "Epoch 9\n",
      "dd??\n",
      "Train Epoch: 9 [32/34345 (0%)]\tLoss: 0.928289\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [352/34345 (1%)]\tLoss: 0.776894\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [672/34345 (2%)]\tLoss: 1.032923\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [992/34345 (3%)]\tLoss: 1.022972\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [1312/34345 (4%)]\tLoss: 0.875436\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [1632/34345 (5%)]\tLoss: 1.018189\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [1952/34345 (6%)]\tLoss: 0.900275\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [2272/34345 (7%)]\tLoss: 0.915581\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [2592/34345 (8%)]\tLoss: 0.824792\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [2912/34345 (8%)]\tLoss: 1.073470\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [3232/34345 (9%)]\tLoss: 0.938639\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [3552/34345 (10%)]\tLoss: 0.846232\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [3872/34345 (11%)]\tLoss: 0.760262\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [4192/34345 (12%)]\tLoss: 0.970633\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [4512/34345 (13%)]\tLoss: 1.236800\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [4832/34345 (14%)]\tLoss: 0.869555\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [5152/34345 (15%)]\tLoss: 0.896295\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [5472/34345 (16%)]\tLoss: 1.018019\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [5792/34345 (17%)]\tLoss: 1.050154\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [6112/34345 (18%)]\tLoss: 0.718576\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [6432/34345 (19%)]\tLoss: 0.897676\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [6752/34345 (20%)]\tLoss: 0.925642\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [7072/34345 (21%)]\tLoss: 0.916340\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [7392/34345 (22%)]\tLoss: 0.961611\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [7712/34345 (22%)]\tLoss: 0.964642\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [8032/34345 (23%)]\tLoss: 0.913211\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [8352/34345 (24%)]\tLoss: 0.855680\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [8672/34345 (25%)]\tLoss: 0.978937\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [8992/34345 (26%)]\tLoss: 1.064132\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [9312/34345 (27%)]\tLoss: 1.013542\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [9632/34345 (28%)]\tLoss: 0.946301\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [9952/34345 (29%)]\tLoss: 0.825680\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [10272/34345 (30%)]\tLoss: 1.111374\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [10592/34345 (31%)]\tLoss: 0.809107\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [10912/34345 (32%)]\tLoss: 0.901097\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [11232/34345 (33%)]\tLoss: 1.129829\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [11552/34345 (34%)]\tLoss: 1.009398\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [11872/34345 (35%)]\tLoss: 0.959592\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [12192/34345 (35%)]\tLoss: 0.924270\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [12512/34345 (36%)]\tLoss: 0.866260\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [12832/34345 (37%)]\tLoss: 0.994963\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [13152/34345 (38%)]\tLoss: 1.091103\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [13472/34345 (39%)]\tLoss: 0.926620\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [13792/34345 (40%)]\tLoss: 1.051674\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [14112/34345 (41%)]\tLoss: 0.909212\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [14432/34345 (42%)]\tLoss: 0.806689\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [14752/34345 (43%)]\tLoss: 1.096141\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [15072/34345 (44%)]\tLoss: 0.904660\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [15392/34345 (45%)]\tLoss: 0.973230\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [15712/34345 (46%)]\tLoss: 0.918641\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [16032/34345 (47%)]\tLoss: 1.078188\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [16352/34345 (48%)]\tLoss: 0.859048\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [16672/34345 (49%)]\tLoss: 0.981270\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [16992/34345 (49%)]\tLoss: 1.025889\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [17312/34345 (50%)]\tLoss: 0.874654\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [17632/34345 (51%)]\tLoss: 0.972396\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [17952/34345 (52%)]\tLoss: 1.021288\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [18272/34345 (53%)]\tLoss: 0.881005\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [18592/34345 (54%)]\tLoss: 1.184758\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [18912/34345 (55%)]\tLoss: 1.097462\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [19232/34345 (56%)]\tLoss: 1.156697\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [19552/34345 (57%)]\tLoss: 0.804863\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [19872/34345 (58%)]\tLoss: 1.124027\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [20192/34345 (59%)]\tLoss: 0.774551\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [20512/34345 (60%)]\tLoss: 0.996601\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [20832/34345 (61%)]\tLoss: 0.939399\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [21152/34345 (62%)]\tLoss: 1.095247\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [21472/34345 (63%)]\tLoss: 0.802492\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [21792/34345 (63%)]\tLoss: 0.890626\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [22112/34345 (64%)]\tLoss: 0.992844\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [22432/34345 (65%)]\tLoss: 0.978092\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [22752/34345 (66%)]\tLoss: 0.974806\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [23072/34345 (67%)]\tLoss: 1.126707\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [23392/34345 (68%)]\tLoss: 0.964021\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [23712/34345 (69%)]\tLoss: 1.024759\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [24032/34345 (70%)]\tLoss: 1.022612\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [24352/34345 (71%)]\tLoss: 0.963934\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [24672/34345 (72%)]\tLoss: 1.112145\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [24992/34345 (73%)]\tLoss: 0.997020\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [25312/34345 (74%)]\tLoss: 0.993870\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [25632/34345 (75%)]\tLoss: 0.741599\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [25952/34345 (76%)]\tLoss: 1.070052\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [26272/34345 (76%)]\tLoss: 1.024984\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [26592/34345 (77%)]\tLoss: 0.885004\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [26912/34345 (78%)]\tLoss: 0.848517\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [27232/34345 (79%)]\tLoss: 0.944003\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [27552/34345 (80%)]\tLoss: 0.996426\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [27872/34345 (81%)]\tLoss: 1.043264\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [28192/34345 (82%)]\tLoss: 0.805688\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [28512/34345 (83%)]\tLoss: 0.791464\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [28832/34345 (84%)]\tLoss: 0.997438\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [29152/34345 (85%)]\tLoss: 0.938639\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [29472/34345 (86%)]\tLoss: 1.020081\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [29792/34345 (87%)]\tLoss: 0.934717\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [30112/34345 (88%)]\tLoss: 1.058639\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [30432/34345 (89%)]\tLoss: 1.016247\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [30752/34345 (90%)]\tLoss: 0.926957\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [31072/34345 (90%)]\tLoss: 1.140398\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [31392/34345 (91%)]\tLoss: 0.996453\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [31712/34345 (92%)]\tLoss: 0.928938\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [32032/34345 (93%)]\tLoss: 0.874727\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [32352/34345 (94%)]\tLoss: 1.015429\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [32672/34345 (95%)]\tLoss: 1.210520\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [32992/34345 (96%)]\tLoss: 0.927391\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [33312/34345 (97%)]\tLoss: 0.781195\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [33632/34345 (98%)]\tLoss: 0.893248\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [33952/34345 (99%)]\tLoss: 0.916050\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 9 [34272/34345 (100%)]\tLoss: 0.897523\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Performance on test set: Average loss: 1.0464, Accuracy: 1170/2210 (53%)\n",
      "Epoch took: 1.299s\n",
      "\n",
      "Example prediction\n",
      "dd??\n",
      "Input sentence: This is incredible! I love it, this is the best chicken I have ever had.\n",
      "Predictions: positive: 0.0596, negative: 0.0004, very positive: 0.9391, very negative: 0.0002, neutral: 0.0007\n",
      "\n",
      "Epoch 10\n",
      "dd??\n",
      "Train Epoch: 10 [32/34345 (0%)]\tLoss: 0.928365\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [352/34345 (1%)]\tLoss: 1.002803\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [672/34345 (2%)]\tLoss: 0.922607\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [992/34345 (3%)]\tLoss: 0.927754\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [1312/34345 (4%)]\tLoss: 0.783438\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [1632/34345 (5%)]\tLoss: 1.114328\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [1952/34345 (6%)]\tLoss: 0.798973\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [2272/34345 (7%)]\tLoss: 0.979071\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [2592/34345 (8%)]\tLoss: 0.944910\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [2912/34345 (8%)]\tLoss: 0.925958\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [3232/34345 (9%)]\tLoss: 0.863405\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [3552/34345 (10%)]\tLoss: 1.015419\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [3872/34345 (11%)]\tLoss: 0.877466\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [4192/34345 (12%)]\tLoss: 1.110560\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [4512/34345 (13%)]\tLoss: 1.079476\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [4832/34345 (14%)]\tLoss: 1.069664\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [5152/34345 (15%)]\tLoss: 0.917516\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [5472/34345 (16%)]\tLoss: 0.994266\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [5792/34345 (17%)]\tLoss: 1.123069\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [6112/34345 (18%)]\tLoss: 0.843496\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [6432/34345 (19%)]\tLoss: 0.974734\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [6752/34345 (20%)]\tLoss: 1.146112\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [7072/34345 (21%)]\tLoss: 1.085479\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [7392/34345 (22%)]\tLoss: 0.943709\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [7712/34345 (22%)]\tLoss: 0.861771\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [8032/34345 (23%)]\tLoss: 0.940416\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [8352/34345 (24%)]\tLoss: 1.050915\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [8672/34345 (25%)]\tLoss: 0.863903\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [8992/34345 (26%)]\tLoss: 0.744200\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [9312/34345 (27%)]\tLoss: 1.091935\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [9632/34345 (28%)]\tLoss: 0.866500\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [9952/34345 (29%)]\tLoss: 1.017845\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [10272/34345 (30%)]\tLoss: 0.893507\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [10592/34345 (31%)]\tLoss: 1.001961\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [10912/34345 (32%)]\tLoss: 0.866254\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [11232/34345 (33%)]\tLoss: 1.269672\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [11552/34345 (34%)]\tLoss: 0.937803\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [11872/34345 (35%)]\tLoss: 0.964845\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [12192/34345 (35%)]\tLoss: 0.795418\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [12512/34345 (36%)]\tLoss: 0.926000\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [12832/34345 (37%)]\tLoss: 1.045894\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [13152/34345 (38%)]\tLoss: 0.884987\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [13472/34345 (39%)]\tLoss: 0.981017\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [13792/34345 (40%)]\tLoss: 0.796192\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [14112/34345 (41%)]\tLoss: 0.933884\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [14432/34345 (42%)]\tLoss: 0.960703\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [14752/34345 (43%)]\tLoss: 0.893853\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [15072/34345 (44%)]\tLoss: 0.985237\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [15392/34345 (45%)]\tLoss: 0.898376\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [15712/34345 (46%)]\tLoss: 0.982000\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [16032/34345 (47%)]\tLoss: 0.959502\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [16352/34345 (48%)]\tLoss: 1.047850\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [16672/34345 (49%)]\tLoss: 1.064311\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [16992/34345 (49%)]\tLoss: 0.875069\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [17312/34345 (50%)]\tLoss: 0.904943\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [17632/34345 (51%)]\tLoss: 0.970108\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [17952/34345 (52%)]\tLoss: 0.828658\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [18272/34345 (53%)]\tLoss: 0.910242\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [18592/34345 (54%)]\tLoss: 0.986839\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [18912/34345 (55%)]\tLoss: 0.935657\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [19232/34345 (56%)]\tLoss: 0.970371\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [19552/34345 (57%)]\tLoss: 0.862752\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [19872/34345 (58%)]\tLoss: 0.924096\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [20192/34345 (59%)]\tLoss: 0.975587\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [20512/34345 (60%)]\tLoss: 0.898405\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [20832/34345 (61%)]\tLoss: 0.996667\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [21152/34345 (62%)]\tLoss: 0.882438\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [21472/34345 (63%)]\tLoss: 1.016502\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [21792/34345 (63%)]\tLoss: 0.804703\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [22112/34345 (64%)]\tLoss: 0.892324\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [22432/34345 (65%)]\tLoss: 0.962117\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [22752/34345 (66%)]\tLoss: 0.834111\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [23072/34345 (67%)]\tLoss: 1.131530\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [23392/34345 (68%)]\tLoss: 1.057053\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [23712/34345 (69%)]\tLoss: 0.685494\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [24032/34345 (70%)]\tLoss: 0.992829\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [24352/34345 (71%)]\tLoss: 0.967283\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [24672/34345 (72%)]\tLoss: 0.876532\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [24992/34345 (73%)]\tLoss: 1.052615\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [25312/34345 (74%)]\tLoss: 0.976368\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [25632/34345 (75%)]\tLoss: 1.068887\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [25952/34345 (76%)]\tLoss: 1.072840\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [26272/34345 (76%)]\tLoss: 0.887522\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [26592/34345 (77%)]\tLoss: 1.201157\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [26912/34345 (78%)]\tLoss: 1.062125\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [27232/34345 (79%)]\tLoss: 0.764395\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [27552/34345 (80%)]\tLoss: 0.916473\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [27872/34345 (81%)]\tLoss: 0.986057\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [28192/34345 (82%)]\tLoss: 1.041678\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [28512/34345 (83%)]\tLoss: 1.101693\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [28832/34345 (84%)]\tLoss: 0.791221\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [29152/34345 (85%)]\tLoss: 1.121846\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [29472/34345 (86%)]\tLoss: 1.132796\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [29792/34345 (87%)]\tLoss: 0.912259\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [30112/34345 (88%)]\tLoss: 1.065028\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [30432/34345 (89%)]\tLoss: 0.886913\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [30752/34345 (90%)]\tLoss: 0.866546\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [31072/34345 (90%)]\tLoss: 0.926633\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [31392/34345 (91%)]\tLoss: 1.194480\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [31712/34345 (92%)]\tLoss: 1.042317\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [32032/34345 (93%)]\tLoss: 0.816343\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [32352/34345 (94%)]\tLoss: 1.000442\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [32672/34345 (95%)]\tLoss: 0.957642\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [32992/34345 (96%)]\tLoss: 1.122100\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [33312/34345 (97%)]\tLoss: 1.026044\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [33632/34345 (98%)]\tLoss: 1.008359\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [33952/34345 (99%)]\tLoss: 1.014640\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Train Epoch: 10 [34272/34345 (100%)]\tLoss: 0.904604\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "dd??\n",
      "Performance on test set: Average loss: 1.0543, Accuracy: 1155/2210 (52%)\n",
      "Epoch took: 1.311s\n",
      "\n",
      "Example prediction\n",
      "dd??\n",
      "Input sentence: This is incredible! I love it, this is the best chicken I have ever had.\n",
      "Predictions: positive: 0.0501, negative: 0.0002, very positive: 0.9489, very negative: 0.0001, neutral: 0.0006\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    print(\"\\nEpoch\", epoch + 1)\n",
    "\n",
    "    train_epoch(\n",
    "        discriminator=discriminator,\n",
    "        data_loader=train_loader,\n",
    "        optimizer=optimizer,\n",
    "        epoch=epoch,\n",
    "        log_interval=log_interval,\n",
    "        device=device,\n",
    "    )\n",
    "    evaluate_performance(data_loader=test_loader, discriminator=discriminator, device=device)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Epoch took: {:.3f}s\".format(end - start))\n",
    "\n",
    "    print(\"\\nExample prediction\")\n",
    "    predict(example_sentence, discriminator, idx2class, cached=cached, device=device)\n",
    "\n",
    "    if save_model:\n",
    "        # torch.save(discriminator.state_dict(),\n",
    "        #           \"{}_discriminator_{}.pt\".format(\n",
    "        #               args.dataset, epoch + 1\n",
    "        #               ))\n",
    "        torch.save(\n",
    "            discriminator.get_classifier().state_dict(),\n",
    "            \"{}_classifier_head_epoch_{}.pt\".format(dataset, epoch + 1),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0918d440-0f01-490c-9845-cabe3e4fd7ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daab5cfd-1020-45e0-9b32-d32793f6be7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20011df0-a818-4c40-b4b6-282f9e3ef66e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9965de08-1043-4552-824c-04afbbc86a2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ceb003-f040-4e4c-932d-579346222410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12343a54-5e9b-482b-871c-9ad7ef3e4b17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7151c16b-6a00-4182-98f1-bcd8b04d7d52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
